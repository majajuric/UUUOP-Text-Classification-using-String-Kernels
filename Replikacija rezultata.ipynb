{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce26ca48",
   "metadata": {},
   "source": [
    "Sveučilište u Zagrebu<br>\n",
    "Fakultet elektrotehnike i računarstva\n",
    "\n",
    "## Uvod u znanost o podacima\n",
    "\n",
    "# Replikacija rezultata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3015166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import random\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049dcb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\majajuric\\anaconda3\\lib\\site-packages (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install swig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3da48",
   "metadata": {},
   "source": [
    "Znanstveni rad opisuje novi način razvrstavanja članaka na temelju jezgrenih funkcija. Jezgrene funkcije predstavljaju umnožak u prostoru značajki. Jezgrene funkcije se koriste za klasifikaciju članaka jer je članke teško vektorizirati, odnosno pretvoriti u vektor značajki.\n",
    "\n",
    "Znanstveni rad opisuje implementaciju jezgre SSK koja navodno ima bolje performanse od standardnih jezgri NGK i WK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26273c9b",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984554e0",
   "metadata": {},
   "source": [
    "Jezgrene funkcije računaju sličnost između dva primjera. Sličnost se računa kao umnožak u prostoru značajki. Primjeri se samo implicitno preslikavaju u prostor značajki i tamo se množe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b2858",
   "metadata": {},
   "source": [
    "### WK kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7428f",
   "metadata": {},
   "source": [
    "Standardni pristup klasifikaciji teksta preslikava tekst u visokodimenzionalni vektor u kojem svaki element vektora označava prisutnost ili nedostatak neke značajke. Ovakav pristup gubi svu informaciju o redoslijedu riječi te zadržava samo informaciju o frekvenciji pojavljivanja pojmova u dokumentu.\n",
    "\n",
    "Npr.\n",
    "s=\"science is organized knowledge\"\n",
    "t=\"wisdom is organized life\"\n",
    "\n",
    "feature vector = [\"science, is, organized, knowledge, wisdom, life]\n",
    "\n",
    "fi_1 = [1, 1, 1, 1, 0, 0]\n",
    "fi_2 = [0, 1, 1, 0, 1, 1]\n",
    "\n",
    "K(s, t) = [1, 1, 1, 1, 0, 0]*[0, 1, 1, 0, 1, 1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f847ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2376018",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk_kernel = lambda x, y: wk(x, y)\n",
    "\n",
    "def wkGmats(trainDocs, testDocs):\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = \"english\", input='content') \n",
    "\n",
    "    train_data_features = vectorizer.fit_transform(trainDocs)\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    tfidf = transformer.fit_transform(train_data_features)\n",
    "    tfidf = tfidf.toarray() \n",
    "\n",
    "    nTrainDocs = len(tfidf)\n",
    "    GmatTrain = np.ones((nTrainDocs,nTrainDocs))\n",
    "\n",
    "    for i in range( 0, nTrainDocs ):\n",
    "        for j in range(0,nTrainDocs):\n",
    "            GmatTrain[i][j] = np.dot(tfidf[i], tfidf[j])\n",
    "            \n",
    "    n_features_train = len(tfidf[0])\n",
    "    \n",
    "    train_data_features = vectorizer.transform(testDocs)\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    tfidfTest = transformer.fit_transform(train_data_features)\n",
    "    tfidfTest = tfidfTest.toarray() \n",
    "\n",
    "    nTestDocs = len(tfidfTest)\n",
    "    GmatTest = np.ones((nTestDocs,nTrainDocs))\n",
    "\n",
    "    for i in range( 0, nTestDocs ):\n",
    "        for j in range(0,nTrainDocs):\n",
    "            GmatTest[i][j] = np.dot(tfidfTest[i], tfidf[j])\n",
    "    \n",
    "    return GmatTrain, GmatTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98cfab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting WK, creating bag of words...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14851129125610232"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wk_train_gram_mat, wk_test_gram_mat] = wkGmats([\"science is organized knowledge\"], [\"wisdom is organized life\"])\n",
    "\n",
    "wk.wk(\"science is organized knowledge\",\"wisdom is organized life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750371e6",
   "metadata": {},
   "source": [
    "### NGK kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e50ab",
   "metadata": {},
   "source": [
    "NGK jezgra koristi n-grams. N-grams daju n susjednih slova nekog stringa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ef40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngk\n",
    "\n",
    "doc1, doc2 = \"science is organized knowledge\", \"wisdom is organized life\"\n",
    "print(ngk.ngk(doc1, doc2))\n",
    "print(ngk.ngk(doc1, doc2, n=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5260b",
   "metadata": {},
   "source": [
    "### SSK kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea4ddb",
   "metadata": {},
   "source": [
    "Cijeli dokument se promatra kao jedan dugačak sequence. Prostor značajki u ovom slučaju je set svih ne nužno susjednih substringova od k simbola. Dva članka su to sličniji što imaju više zajedničkih takvih substringova.\n",
    "\n",
    "Sličnost se računa u ovisnosti o lambdi koja mjeri težinu u ovisnosti u duljini i uzastopnosti charactera svakog subsequenca iz jednog dokumenta u drugom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ab86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a737b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssk_compute_train_gram(docs, kernel=None):\n",
    "    n = len(docs)\n",
    "    gram = np.ones((n, n))\n",
    "    for x in range(n):\n",
    "        print('{0:.2f}%'.format(x / n))\n",
    "        for y in range(x + 1, n):\n",
    "            gram[x, y] = kernel(docs[x], docs[y])\n",
    "            gram[y, x] = gram[x, y]\n",
    "    return gram  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3670d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssk_compute_test_gram(test, train, kernel=None):\n",
    "    gram = np.zeros((len(test), len(train)))\n",
    "    for x in range(len(test)):\n",
    "        print('{0:.2f}%'.format(x / len(test)))\n",
    "        for y in range(len(train)):\n",
    "            gram[x, y] = kernel(test[x], train[y])\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a97876a",
   "metadata": {},
   "source": [
    "### Priprema podataka\n",
    "U članku piše da su sve riječi u body-jima svih članaka pretvorene u lowercase. Također, uklonjene su sve *stopwords*, a interpunkcijski znakovi zamijenjeni su razmacima. Također, bitni su nam samo stupci TOPICS i BODY tako da ostale možemo izbaciti.\n",
    "\n",
    "U članku piše da su zadržali samo stem riječi. Pokušala sam to napraviti, ali javlja neku grešku s nltk --> pokušati opet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7351c0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICS</th>\n",
       "      <th>BODY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cocoa']</td>\n",
       "      <td>showers continued throughout week bahia cocoa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['grain', 'wheat', 'corn', 'barley', 'oat', 's...</td>\n",
       "      <td>u agriculture department reported farmer owned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['veg-oil', 'linseed', 'lin-oil', 'soy-oil', '...</td>\n",
       "      <td>argentine grain board figures show crop regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['earn']</td>\n",
       "      <td>champion products inc said board directors app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['acq']</td>\n",
       "      <td>computer terminal systems inc said completed s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              TOPICS  \\\n",
       "0                                          ['cocoa']   \n",
       "1  ['grain', 'wheat', 'corn', 'barley', 'oat', 's...   \n",
       "2  ['veg-oil', 'linseed', 'lin-oil', 'soy-oil', '...   \n",
       "3                                           ['earn']   \n",
       "4                                            ['acq']   \n",
       "\n",
       "                                                BODY  \n",
       "0  showers continued throughout week bahia cocoa ...  \n",
       "1  u agriculture department reported farmer owned...  \n",
       "2  argentine grain board figures show crop regist...  \n",
       "3  champion products inc said board directors app...  \n",
       "4  computer terminal systems inc said completed s...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clanci_stripped = pd.read_csv(\"clanci_stripped.csv\", index_col = 0)\n",
    "clanci_stripped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176c772",
   "metadata": {},
   "source": [
    "Razdvajamo BODY-je po TOPICS-ima tako da u TOPICS nije lista nego samo jedna vrijednost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "393bdfb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICS</th>\n",
       "      <th>BODY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'cocoa'</td>\n",
       "      <td>showers continued throughout week bahia cocoa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'grain'</td>\n",
       "      <td>agriculture department reported farmer owned r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'wheat'</td>\n",
       "      <td>agriculture department reported farmer owned r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'corn'</td>\n",
       "      <td>agriculture department reported farmer owned r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'barley'</td>\n",
       "      <td>agriculture department reported farmer owned r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'oat'</td>\n",
       "      <td>agriculture department reported farmer owned r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'sorghum'</td>\n",
       "      <td>agriculture department reported farmer owned r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'veg-oil'</td>\n",
       "      <td>argentine grain board figures show crop regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'linseed'</td>\n",
       "      <td>argentine grain board figures show crop regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'lin-oil'</td>\n",
       "      <td>argentine grain board figures show crop regist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TOPICS                                               BODY\n",
       "0     'cocoa'  showers continued throughout week bahia cocoa ...\n",
       "1     'grain'  agriculture department reported farmer owned r...\n",
       "2     'wheat'  agriculture department reported farmer owned r...\n",
       "3      'corn'  agriculture department reported farmer owned r...\n",
       "4    'barley'  agriculture department reported farmer owned r...\n",
       "5       'oat'  agriculture department reported farmer owned r...\n",
       "6   'sorghum'  agriculture department reported farmer owned r...\n",
       "7   'veg-oil'  argentine grain board figures show crop regist...\n",
       "8   'linseed'  argentine grain board figures show crop regist...\n",
       "9   'lin-oil'  argentine grain board figures show crop regist..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clanci_stripped = clanci_stripped.loc[:, [\"TOPICS\", \"BODY\"]]\n",
    "clanci_stripped = clanci_stripped.loc[clanci_stripped.TOPICS.notnull(), :]\n",
    "#print(clanci.head(n=10))\n",
    "\n",
    "mapa = {'TOPICS': [], 'BODY': []}\n",
    "for index, row in clanci_stripped.iterrows():\n",
    "    topics = row.TOPICS.split(\",\")\n",
    "    for topic in topics:\n",
    "        topic = topic.replace('[', \"\")\n",
    "        topic = topic.replace(\"]\", \"\")\n",
    "        if mapa['TOPICS'] is None:\n",
    "            mapa['TOPICS'] = [topic]\n",
    "        else:\n",
    "            mapa['TOPICS'].append(topic)\n",
    "            \n",
    "        body = row.BODY\n",
    "        unfiltered_body = \"\"\n",
    "        for word in body.split(\" \"):\n",
    "            if len(word) > 3:\n",
    "                unfiltered_body+=word\n",
    "                unfiltered_body+=\" \"\n",
    "        filtered_body = \"\"\n",
    "        for character in unfiltered_body:\n",
    "            if (character.isalnum()) or (character == ' '):\n",
    "                filtered_body += character\n",
    "        filtered_body = filtered_body.replace('[^a-zA-Z]', \" \")\n",
    "        filtered_body = filtered_body.replace(' [ ]+', ' ')\n",
    "        \n",
    "        if mapa['BODY'] is None:\n",
    "            mapa['BODY'] = [filtered_body]\n",
    "        else:\n",
    "            mapa['BODY'].append(filtered_body)\n",
    "            \n",
    "dataframe = pd.DataFrame(mapa)\n",
    "dataframe.to_csv(\"clanci_split.csv\")\n",
    "dataframe.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca78e2",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5bf472",
   "metadata": {},
   "source": [
    "Ciljevi eksperimenata su:\n",
    "- proučavati utjecaj promjene parametara k(duljina) i $\\lambda$(težina)\n",
    "- uočiti prednosti kombiniranja različitih jezgri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aaf77b",
   "metadata": {},
   "source": [
    "### Podjela podataka u train i test skup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6d1c8",
   "metadata": {},
   "source": [
    "Eksperimenti su provedeni samo na dijelu Reuters seta. U članku piše da je subset bio veličine 470 dokumenata, od čega je 380 bilo korišteno za treniranje, a 90 za ispitivanje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bad638",
   "metadata": {},
   "source": [
    "U eksperimentu su odabrane kategorije \"earn\", \"acq\", \"crude\" i \"corn\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "earn_clanci = dataframe[dataframe.TOPICS.str.contains(\"earn\")]\n",
    "acq_clanci = dataframe[dataframe.TOPICS.str.contains(\"acq\")]\n",
    "crude_clanci = dataframe[dataframe.TOPICS.str.contains(\"crude\")]\n",
    "corn_clanci = dataframe[dataframe.TOPICS.str.contains(\"corn\")]\n",
    "\n",
    "clanci = [earn_clanci, acq_clanci, crude_clanci, corn_clanci]\n",
    "clanci.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1e278",
   "metadata": {},
   "source": [
    "Navedeno je da je broj članaka za pojedinu kategoriju za učenje (ispitivanje) sljedeći:\n",
    "1. earn 152 (40)\n",
    "2. acquisition 114 (25)\n",
    "3. crude 76 (15)\n",
    "4. corn 38 (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f154e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def give_train_test_split(give_train):\n",
    "    [earn_tr, earn_te] = train_test_split(earn_clanci, train_size=152/len(earn_clanci), test_size=40/len(earn_clanci))\n",
    "    [acq_tr, acq_te] = train_test_split(acq_clanci, train_size=114/len(acq_clanci), test_size=25/len(acq_clanci))\n",
    "    [crude_tr, crude_te] = train_test_split(crude_clanci, train_size=76/len(crude_clanci), test_size=15/len(crude_clanci))\n",
    "    [corn_tr, corn_te] = train_test_split(corn_clanci, train_size=38/len(corn_clanci), test_size=10/len(corn_clanci))\n",
    "\n",
    "    y_train = []\n",
    "    y_train.extend(['earn' for i in range(0, len(earn_tr))])\n",
    "    y_train.extend(['acq' for i in range(0, len(acq_tr))])\n",
    "    y_train.extend(['crude' for i in range(0, len(crude_tr))])\n",
    "    y_train.extend(['corn' for i in range(0, len(corn_tr))])\n",
    "    y_train = np.array(y_train)\n",
    "    #print(y_train)\n",
    "\n",
    "    y_test = []\n",
    "    y_test.extend(['earn' for i in range(0, len(earn_te))])\n",
    "    y_test.extend(['acq' for i in range(0, len(acq_te))])\n",
    "    y_test.extend(['crude' for i in range(0, len(crude_te))])\n",
    "    y_test.extend(['corn' for i in range(0, len(corn_te))])\n",
    "    y_test = np.array(y_test)\n",
    "    #print(y_test)\n",
    "\n",
    "    clanci_test = [earn_te, acq_te, crude_te, corn_te]\n",
    "    clanci_test = pd.concat(clanci_test)\n",
    "    clanci_train = [earn_tr, acq_tr, crude_tr, corn_tr]\n",
    "    clanci_train = pd.concat(clanci_train)\n",
    "    #clanci_train #.to_csv(\"clanci_train.csv\")\n",
    "    \n",
    "    earn_test=[]\n",
    "    acq_test=[]\n",
    "    crude_test=[]\n",
    "    corn_test=[]\n",
    "    for index, row in earn_te.iterrows():\n",
    "        earn_test.append(row['BODY'])\n",
    "    for index, row in acq_te.iterrows():\n",
    "        acq_test.append(row['BODY'])\n",
    "    for index, row in crude_te.iterrows():\n",
    "        crude_test.append(row['BODY'])\n",
    "    for index, row in corn_te.iterrows():\n",
    "        corn_test.append(row['BODY'])\n",
    "    \n",
    "    treniranje_parovi = []\n",
    "    treniranje = []\n",
    "    i = 0\n",
    "    for index, row in clanci_train.iterrows():\n",
    "        par = []\n",
    "        par = [row['BODY'], y_train[i]]\n",
    "        treniranje.append(row['BODY'])\n",
    "        treniranje_parovi.append(par)\n",
    "\n",
    "    testiranje = []\n",
    "    for index, row in clanci_test.iterrows():\n",
    "        testiranje.append(row['BODY'])\n",
    "    \n",
    "    if give_train:\n",
    "        return [treniranje, y_train]\n",
    "    else:\n",
    "        return [earn_test, acq_test, crude_test, corn_test, testiranje, y_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee86bf1",
   "metadata": {},
   "source": [
    "### Effectiveness of Varying Sequence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc05ef",
   "metadata": {},
   "source": [
    "U ovom dijelu promatramo kako parametar duljine subsequenca, k, utječe na točnost modela. Za svaku vrijednost k, eksperiment je proveden 10 puta i onda su dobivene vrijednosti mean i sd. Lambda je postavljen na 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad27292",
   "metadata": {},
   "source": [
    "Kako je računanje za SSK jako sporo, kod njega sam izostavila provođenje eksperimenta 10 puta pa se on provodi samo jednom. Za NGK se provodi 10 puta i vrijednosti evaluacije rezultata su uprosječene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e654a",
   "metadata": {},
   "source": [
    "Stvorimo listu u koju stavljamo [category, ime ljuske, length, f1_mean, f1_std, precision_mean, precision_std, recall_mean, recall_std]. Od te liste kasnije stvorimo dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f4615",
   "metadata": {},
   "source": [
    "#### Evaluacija SSK jezgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a9cda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ispisuje tablicu kao sto je u radu\n",
    "def ssk_evaluation(category, k_range=[5], lambd_range=[0.5]):\n",
    "    rezultat_lista = []\n",
    "\n",
    "    print(\"Starting SSK evaluation for {}...\".format(category))\n",
    "    for k in k_range:\n",
    "        for lambd in lambd_range:\n",
    "            print(\"\\tk = {}, lambda = {}\".format(k, lambd))\n",
    "            lista_u_ovom_koraku = []\n",
    "            lista_u_ovom_koraku.append(category)\n",
    "            lista_u_ovom_koraku.append(\"SSK\")\n",
    "            f1 = []\n",
    "            precision = []\n",
    "            recall = []\n",
    "            #for i in range(0, 10):\n",
    "            [trening, y_trening] = give_train_test_split(True)\n",
    "            \n",
    "            #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "            treniranje = []\n",
    "            y_train = []\n",
    "            for i in range(0, 60):\n",
    "                r = random.randint(0, len(trening)-1)\n",
    "                treniranje.append(trening[r])\n",
    "                y_train.append(y_trening[r])\n",
    "            \n",
    "            ssk_kernel = lambda x, y: ssk.ssk(x, y, k, lambd)\n",
    "            train_gram = ssk_compute_train_gram(treniranje, kernel=ssk_kernel)\n",
    "            [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "            if(category==\"earn\"):\n",
    "                X_test = earn_test\n",
    "                y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "            elif category == \"acq\":\n",
    "                X_test = acq_test\n",
    "                y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "            elif category == \"crude\":\n",
    "                X_test = crude_test\n",
    "                y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "            else:\n",
    "                X_test = corn_test\n",
    "                y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "\n",
    "            test_gram = ssk_compute_test_gram(X_test, treniranje, kernel=ssk_kernel)\n",
    "            clf = SVC(kernel='precomputed')\n",
    "            clf.fit(train_gram, y_train)\n",
    "            # predikcija\n",
    "            y_pred = clf.predict(test_gram)\n",
    "            f1.append(f1_score(y_true, y_pred, average='micro'))\n",
    "            precision.append(precision_score(y_true, y_pred, average='micro'))\n",
    "            recall.append(recall_score(y_true, y_pred, average='micro'))\n",
    "            \n",
    "        if len(k_range) == 1:\n",
    "            lista_u_ovom_koraku.append(lambd)\n",
    "        else:\n",
    "            lista_u_ovom_koraku.append(k)\n",
    "            \n",
    "        lista_u_ovom_koraku.append(round(np.mean(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(recall), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(recall), 3))\n",
    "        \n",
    "        rezultat_lista.append(lista_u_ovom_koraku)\n",
    "        lista_u_ovom_koraku.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    print(\"End of SSK evaluation\\n\")\n",
    "    return rezultat_lista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014de0d",
   "metadata": {},
   "source": [
    "#### Evaluacija NGK jezgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f093fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngk_evaluation(category, k_range=[5]):\n",
    "    rezultat_lista = []\n",
    "    \n",
    "    print(\"Starting NGK evaluation for {}...\".format(category))\n",
    "    for k in k_range:\n",
    "        lista_u_ovom_koraku = []\n",
    "        lista_u_ovom_koraku.append(category)\n",
    "        lista_u_ovom_koraku.append(\"NGK\")\n",
    "        print(\"\\t k =\", k)\n",
    "        f1 = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for i in range(0, 10):\n",
    "            #treniranje jezgre\n",
    "            \n",
    "            [trening, y_trening] = give_train_test_split(True)\n",
    "            \n",
    "            #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "            treniranje = []\n",
    "            y_train = []\n",
    "            for i in range(0, 60):\n",
    "                r = random.randint(0, len(trening))\n",
    "                treniranje.append(trening[r])\n",
    "                y_train.append(y_trening[r])\n",
    "        \n",
    "            [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "            if(category==\"earn\"):\n",
    "                X_test = earn_test\n",
    "                y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "            elif category == \"acq\":\n",
    "                X_test = acq_test\n",
    "                y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "            elif category == \"crude\":\n",
    "                X_test = crude_test\n",
    "                y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "            else:\n",
    "                X_test = corn_test\n",
    "                y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "\n",
    "            train_gram, test_gram = ngk.ngkGmats(treniranje, X_test, n=k)\n",
    "            clf = SVC(kernel='precomputed')\n",
    "            clf.fit(train_gram, y_train)\n",
    "            y_pred = clf.predict(test_gram)\n",
    "            f1.append(round(f1_score(y_true, y_pred, average='micro'), 3))\n",
    "            precision.append(round(precision_score(y_true, y_pred, average='micro'), 3))\n",
    "            recall.append(round(recall_score(y_true, y_pred, average='micro'), 3))\n",
    "        \n",
    "        if len(k_range) == 1:\n",
    "            lista_u_ovom_koraku.append(0)\n",
    "        else:\n",
    "            lista_u_ovom_koraku.append(k)\n",
    "        lista_u_ovom_koraku.append(round(np.mean(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(recall), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(recall), 3))\n",
    "        rezultat_lista.append(lista_u_ovom_koraku)\n",
    "        #print(lista_u_ovom_koraku)\n",
    "    print(\"End of NGK evaluation\\n\")\n",
    "    return rezultat_lista\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef003559",
   "metadata": {},
   "source": [
    "#### Evaluacija WK jezgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "028e592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wk_evaluation(category):\n",
    "    rezultat_lista = []\n",
    "    rezultat_lista.append(category)\n",
    "    rezultat_lista.append(\"WK\")\n",
    "\n",
    "    f1 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    print(\"Starting WK evaluation for {}...\".format(category))\n",
    "    for i in range(0, 10):\n",
    "        #print(category, i)\n",
    "        #treniranje jezgre\n",
    "        \n",
    "        [trening, y_trening] = give_train_test_split(True)\n",
    "            \n",
    "        #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "        treniranje = []\n",
    "        y_train = []\n",
    "        for i in range(0, 60):\n",
    "            r = random.randint(0, len(trening))\n",
    "            treniranje.append(trening[r])\n",
    "            y_train.append(y_trening[r])\n",
    "        \n",
    "        [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "        if(category==\"earn\"):\n",
    "            X_test = earn_test\n",
    "            y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "        elif category == \"acq\":\n",
    "            X_test = acq_test\n",
    "            y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "        elif category == \"crude\":\n",
    "            X_test = crude_test\n",
    "            y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "        else:\n",
    "            X_test = corn_test\n",
    "            y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "\n",
    "        wk_kernel = lambda x, y: wk(x, y)\n",
    "        [wk_train_gram, wk_test_gram] = wkGmats(treniranje, X_test)\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(wk_train_gram, y_train)\n",
    "        y_pred = clf.predict(wk_test_gram)\n",
    "        f1.append(round(f1_score(y_true, y_pred, average='micro'), 3))\n",
    "        precision.append(round(precision_score(y_true, y_pred, average='micro'), 3))\n",
    "        recall.append(round(recall_score(y_true, y_pred, average='micro'), 3))\n",
    "        \n",
    "    rezultat_lista.append(0)\n",
    "    rezultat_lista.append(round(np.mean(f1), 3))\n",
    "    rezultat_lista.append(round(np.std(f1), 3))\n",
    "    rezultat_lista.append(round(np.mean(precision), 3))\n",
    "    rezultat_lista.append(round(np.std(precision), 3))\n",
    "    rezultat_lista.append(round(np.mean(recall), 3))\n",
    "    rezultat_lista.append(round(np.std(recall), 3))\n",
    "    print(\"End of WK evaluation\\n\")\n",
    "    return [rezultat_lista]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "21779125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NGK evaluation for earn...\n",
      "\t k = 3\n",
      "\t k = 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [135]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     varying_sequence_length \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mime ljuske\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     53\u001b[0m     varying_sequence_length\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarying_sequence_length.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mevaluation_for_varying_sequence_lengths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [135]\u001b[0m, in \u001b[0;36mevaluation_for_varying_sequence_lengths\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m varying_sequence_length \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mime ljuske\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m varying_sequence_length\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarying_sequence_length.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m rezultat \u001b[38;5;241m=\u001b[39m \u001b[43mngk_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m varying_sequence_length \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mime ljuske\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m varying_sequence_length\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarying_sequence_length.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [101]\u001b[0m, in \u001b[0;36mngk_evaluation\u001b[1;34m(category, k_range)\u001b[0m\n\u001b[0;32m     27\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m corn_test\n\u001b[0;32m     28\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(corn_test))])\n\u001b[1;32m---> 30\u001b[0m train_gram, test_gram \u001b[38;5;241m=\u001b[39m \u001b[43mngk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngkGmats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtreniranje\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(train_gram, y_train)\n",
      "File \u001b[1;32m~\\Documents\\FER\\Uvod u znanost o podacima\\Text Classification using String Kernels\\ngk.py:71\u001b[0m, in \u001b[0;36mngkGmats\u001b[1;34m(trainDocs, testDocs, n, mode, norm)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( \u001b[38;5;241m0\u001b[39m, nTrainDocs ):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,nTrainDocs):\n\u001b[1;32m---> 71\u001b[0m         ngkTrainKmat[i][j] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatVecsTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatVecsTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m ngkTestKmat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((nTestDocs,nTrainDocs))\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( \u001b[38;5;241m0\u001b[39m, nTestDocs ):\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# k_range=[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "def evaluation_for_varying_sequence_lengths(): \n",
    "    rezultat = []\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\")\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"earn\", k_range=[3, 5, 7, 14])\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"earn\", k_range=[3, 5, 7, 14])\n",
    "    #varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"earn\")\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"acq\", k_range=[3, 5, 7, 14])\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"acq\", k_range=[3, 5, 7, 14])\n",
    "    #varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"acq\")\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"crude\", k_range=[3, 5, 7, 14])\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"crude\", k_range=[3, 5, 7, 14])\n",
    "    #varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"crude\")\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"corn\", k_range=[3, 5, 7, 14])\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"corn\", k_range=[3, 5, 7, 14])\n",
    "    #varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "\n",
    "    rezultat = wk_evaluation(\"corn\")\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(\"varying_sequence_length.csv\", mode='a', header=False)\n",
    "    \n",
    "evaluation_for_varying_sequence_lengths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd496e",
   "metadata": {},
   "source": [
    "#### Usporedba rezultata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c5747",
   "metadata": {},
   "source": [
    "Radi brzine izvođenja, nisam računala za sve k kao što je u znanstvenom radu. Ali se može uočiti da SSK najbolje radi za male i srednje velike k (otprilike 4-7). Parametar k može se postaviti unaprijed unakrsnom provjerom tako da maksimizira točnost (minimizira pogrešku) na skupu za provjeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "96969eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MajaJuric\\AppData\\Local\\Temp\\ipykernel_5732\\3951764689.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>ime ljuske</th>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [f1_mean, f1_std, precision_mean, precision_std, recall_mean, recall_std]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "rezultat = pd.read_csv(\"varying_sequence_length.csv\", index_col=[0, 1, 2, 3])\n",
    "display(rezultat)\n",
    "#pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ee940",
   "metadata": {},
   "source": [
    "### Effectiveness of Varying Weight Decay Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587c1e8",
   "metadata": {},
   "source": [
    "Sada ispitujemo model za promjenjive vrijednosti lambde. Lambda upravlja \"kažnjavanjem\" ne-susjednih substringova. Što su stringovi \"ne-susjedniji\" u člancima, to su više kažnjeni.\n",
    "\n",
    "Pozivamo iste funkcije kao i za Varying Sequence Length, ali ovaj puta predajemo niz lambdi (weight decay factor) za koje testiramo model.\n",
    "\n",
    "Za svaku vrijednost lambda, eksperiment je proveden 10 puta i onda su dobivene vrijednosti mean i sd. Parametar k je postavljen na 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4b39d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NGK evaluation for earn...\n",
      "\t k = 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [137]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     varying_weight_decay \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mime ljuske\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     53\u001b[0m     varying_weight_decay\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarying_weight_decay.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mevaluation_for_varying_weight_decay_factors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [137]\u001b[0m, in \u001b[0;36mevaluation_for_varying_weight_decay_factors\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m varying_weight_decay \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mime ljuske\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m varying_weight_decay\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarying_weight_decay.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m rezultat \u001b[38;5;241m=\u001b[39m \u001b[43mngk_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m varying_weight_decay \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mime ljuske\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m varying_weight_decay\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarying_weight_decay.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [101]\u001b[0m, in \u001b[0;36mngk_evaluation\u001b[1;34m(category, k_range)\u001b[0m\n\u001b[0;32m     27\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m corn_test\n\u001b[0;32m     28\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(corn_test))])\n\u001b[1;32m---> 30\u001b[0m train_gram, test_gram \u001b[38;5;241m=\u001b[39m \u001b[43mngk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngkGmats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtreniranje\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(train_gram, y_train)\n",
      "File \u001b[1;32m~\\Documents\\FER\\Uvod u znanost o podacima\\Text Classification using String Kernels\\ngk.py:71\u001b[0m, in \u001b[0;36mngkGmats\u001b[1;34m(trainDocs, testDocs, n, mode, norm)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( \u001b[38;5;241m0\u001b[39m, nTrainDocs ):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,nTrainDocs):\n\u001b[1;32m---> 71\u001b[0m         ngkTrainKmat[i][j] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatVecsTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatVecsTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m ngkTestKmat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((nTestDocs,nTrainDocs))\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( \u001b[38;5;241m0\u001b[39m, nTestDocs ):\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rezultat_lista = []\n",
    "def evaluation_for_varying_weight_decay_factors():\n",
    "    rezultat = []\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\")\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"earn\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"earn\", lambd_range=[0.01, 0.05, 0.3, 0.7])\n",
    "    #varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"earn\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"acq\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"acq\", lambd_range=[0.01, 0.05, 0.3, 0.7])\n",
    "    #varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"acq\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"crude\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"crude\", lambd_range=[0.01, 0.05, 0.3, 0.7])\n",
    "    #varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"crude\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"corn\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    #rezultat = ssk_evaluation(\"corn\", lambd_range=[0.01, 0.05, 0.3, 0.7])\n",
    "    #varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    #varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"corn\")\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(\"varying_weight_decay.csv\", mode='a', header=False)\n",
    "\n",
    "evaluation_for_varying_weight_decay_factors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0215d2b",
   "metadata": {},
   "source": [
    "#### Usporedba rezultata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2708faf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd3e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rezultat = pd.read_csv(\"varying_weight_decay.csv\", index_col=[0, 1, 2, 3])\n",
    "display(rezultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2ccae",
   "metadata": {},
   "source": [
    "### Effectiveness of Combining Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc44da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd539ee",
   "metadata": {},
   "source": [
    "#### Combining NGK and SSK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "27cafebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "lambd = 0.5\n",
    "def NGK_SSK_comb_evaluation(category):\n",
    "    rezultat_lista = []\n",
    "    w_ng_list = [1, 0.5, 0.8, 0.9] #[1, 0, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    w_sk_list = [0, 0.5, 0.2, 0.1] #[0, 1, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    print(\"Starting NGK_SSK_COMB evaluation...\")\n",
    "    for i in range(0, len(w_ng_list)):\n",
    "        w_ng = w_ng_list[i]\n",
    "        w_sk = w_sk_list[i]\n",
    "        lista_u_ovom_koraku = []\n",
    "        lista_u_ovom_koraku.append(category)\n",
    "        lista_u_ovom_koraku.append(w_ng)\n",
    "        lista_u_ovom_koraku.append(w_sk)\n",
    "        print(\"\\tw_ng={} w_sk={}\".format(w_ng, w_sk))\n",
    "        f1 = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        #for i in range(0, 10):\n",
    "        #treniranje jezgre\n",
    "        \n",
    "        [trening, y_trening] = give_train_test_split(True)\n",
    "            \n",
    "        #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "        treniranje = []\n",
    "        y_train = []\n",
    "        for i in range(0, 60):\n",
    "            r = random.randint(0, len(trening)-1)\n",
    "            treniranje.append(trening[r])\n",
    "            y_train.append(y_trening[r])\n",
    "            \n",
    "        [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "        if(category==\"earn\"):\n",
    "            X_test = earn_test\n",
    "            y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "        elif category == \"acq\":\n",
    "            X_test = acq_test\n",
    "            y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "        elif category == \"crude\":\n",
    "            X_test = crude_test\n",
    "            y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "        else:\n",
    "            X_test = corn_test\n",
    "            y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "            \n",
    "        ssk_kernel = lambda x, y: ssk.ssk(x, y, 5, 0.5)\n",
    "        ssk_train_gram = ssk_compute_train_gram(treniranje, kernel=ssk_kernel)\n",
    "        ssk_test_gram = ssk_compute_test_gram(X_test, treniranje, kernel=ssk_kernel)\n",
    "        ngk_train_gram, ngk_test_gram = ngk.ngkGmats(treniranje, X_test, n=5)\n",
    "            \n",
    "        test_gram = ngk_test_gram*w_ng + ssk_test_gram*w_sk\n",
    "        train_gram = ngk_train_gram*w_ng + ssk_train_gram*w_sk\n",
    "            \n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(train_gram, y_train)\n",
    "        y_pred = clf.predict(test_gram)\n",
    "        f1.append(round(f1_score(y_true, y_pred, average='micro')), 3)\n",
    "        precision.append(round(precision_score(y_true, y_pred, average='micro')), 3)\n",
    "        recall.append(round(recall_score(y_true, y_pred, average='micro')), 3)\n",
    "        # kraj for i petlje\n",
    "        \n",
    "        lista_u_ovom_koraku.append(round(np.mean(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(recall), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(recall), 3))\n",
    "        #print(lista_u_ovom_koraku)\n",
    "        rezultat_lista.append(lista_u_ovom_koraku)\n",
    "        \n",
    "    print(\"Ending NGK_SSK_COMB evaluation\\n\")\n",
    "    return rezultat_lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4636a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NGK_SSK_COMB evaluation...\n",
      "\tw_ng=1 w_sk=0\n",
      "60\n",
      "0.00%\n",
      "0.02%\n",
      "0.03%\n",
      "0.05%\n",
      "0.07%\n",
      "0.08%\n",
      "0.10%\n",
      "0.12%\n",
      "0.13%\n",
      "0.15%\n",
      "0.17%\n",
      "0.18%\n",
      "0.20%\n",
      "0.22%\n",
      "0.23%\n",
      "0.25%\n",
      "0.27%\n",
      "0.28%\n",
      "0.30%\n",
      "0.32%\n",
      "0.33%\n",
      "0.35%\n",
      "0.37%\n",
      "0.38%\n",
      "0.40%\n",
      "0.42%\n",
      "0.43%\n",
      "0.45%\n",
      "0.47%\n",
      "0.48%\n",
      "0.50%\n",
      "0.52%\n",
      "0.53%\n",
      "0.55%\n",
      "0.57%\n",
      "0.58%\n",
      "0.60%\n",
      "0.62%\n",
      "0.63%\n",
      "0.65%\n",
      "0.67%\n",
      "0.68%\n",
      "0.70%\n",
      "0.72%\n",
      "0.73%\n",
      "0.75%\n",
      "0.77%\n",
      "0.78%\n",
      "0.80%\n",
      "0.82%\n",
      "0.83%\n",
      "0.85%\n",
      "0.87%\n",
      "0.88%\n",
      "0.90%\n",
      "0.92%\n",
      "0.93%\n",
      "0.95%\n",
      "0.97%\n",
      "0.98%\n",
      "0.00%\n",
      "0.03%\n",
      "0.05%\n",
      "0.07%\n",
      "0.10%\n",
      "0.12%\n",
      "0.15%\n",
      "0.17%\n",
      "0.20%\n",
      "0.23%\n",
      "0.25%\n",
      "0.28%\n",
      "0.30%\n",
      "0.33%\n",
      "0.35%\n",
      "0.38%\n",
      "0.40%\n",
      "0.42%\n",
      "0.45%\n",
      "0.47%\n",
      "0.50%\n"
     ]
    }
   ],
   "source": [
    "def evaluation_for_combining_ngk_and_ssk():\n",
    "    rezultat = []\n",
    "    combining_ngk_and_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combining_ngk_and_ssk.to_csv(\"combining_ngk_and_ssk.csv\")\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"earn\")\n",
    "    combining_ngk_and_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combining_ngk_and_ssk.to_csv(\"combining_ngk_and_ssk.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"acq\")\n",
    "    combining_ngk_and_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combining_ngk_and_ssk.to_csv(\"combining_ngk_and_ssk.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"crude\")\n",
    "    combining_ngk_and_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combining_ngk_and_ssk.to_csv(\"combining_ngk_and_ssk.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"corn\")\n",
    "    combining_ngk_and_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combining_ngk_and_ssk.to_csv(\"combining_ngk_and_ssk.csv\", mode='a', header=False)\n",
    "\n",
    "evaluation_for_combining_ngk_and_ssk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db0429",
   "metadata": {},
   "source": [
    "#### Combining SSK with different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSK_lambda_comb_evaluation(category):\n",
    "    rezultat_lista = []\n",
    "    lambda_1_list = [0.05, 0.5, 0.05]\n",
    "    lambda_2_list = [0.0, 0.0, 0.5]\n",
    "    print(\"Starting combining SSK with differend lambdas...\")\n",
    "    for i in len(lambda_1_list):\n",
    "        lambda1 = lambda_1_list[i]\n",
    "        lambda2 = lambda_2_list[i]\n",
    "        lista_u_ovom_koraku = []\n",
    "        lista_u_ovom_koraku.append(category)\n",
    "        lista_u_ovom_koraku.append(lambda1)\n",
    "        lista_u_ovom_koraku.append(lambda2)\n",
    "        print(\"lambda1={} lambda2={}\".format(lambda1, lambda2))\n",
    "        f1 = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for i in range(0, 10):\n",
    "            #treniranje jezgre\n",
    "            [treniranje, y_train] = give_train_test_split(True)\n",
    "            treniranje = treniranje[0:60] # uzimam samo dio jer je inače previše i predugo traje\n",
    "            y_train = y_train[0:60]\n",
    "            [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "            if(category==\"earn\"):\n",
    "                X_test = earn_test\n",
    "                y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "            elif category == \"acq\":\n",
    "                X_test = acq_test\n",
    "                y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "            elif category == \"crude\":\n",
    "                X_test = crude_test\n",
    "                y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "            else:\n",
    "                X_test = corn_test\n",
    "                y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "            \n",
    "            ssk_kernel_1 = lambda x, y: ssk.ssk(x, y, 5, lambda1)\n",
    "            ssk_kernel_2 = lambda x, y: ssk.ssk(x, y, 5, lambda2)\n",
    "            ssk_train_gram_1 = ssk_compute_train_gram(treniranje, kernel=ssk_kernel_1)\n",
    "            ssk_train_gram_2 = ssk_compute_train_gram(treniranje, kernel=ssk_kernel_2)\n",
    "            ssk_test_gram_1 = ssk_compute_test_gram(X_test, treniranje, kernel=ssk_kernel_1)\n",
    "            ssk_test_gram_2 = ssk_compute_test_gram(X_test, treniranje, kernel=ssk_kernel_2)\n",
    "            \n",
    "            train_gram = ssk_train_gram_1 + ssk_train_gram_2\n",
    "            test_gram = ssk_test_gram_1 + ssk_test_gram_2\n",
    "        \n",
    "            clf = SVC(kernel='precomputed')\n",
    "            clf.fit(train_gram, y_train)\n",
    "            y_pred = clf.predict(test_gram)\n",
    "            f1.append(f1_score(y_true, y_pred, average='micro'))\n",
    "            precision.append(precision_score(y_true, y_pred, average='micro'))\n",
    "            recall.append(recall_score(y_true, y_pred, average='micro'))\n",
    "        \n",
    "        lista_u_ovom_koraku.append(round(np.mean(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(recall), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(recall), 3))\n",
    "        #print(lista_u_ovom_koraku)\n",
    "        rezultat_lista.append(lista_u_ovom_koraku)\n",
    "    \n",
    "    print(\"End of combining SSK with differend lambdas\\n\")\n",
    "    return rezultat_lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_for_combining_lambda_ssk():\n",
    "    rezultat = []\n",
    "    combininig_lambda_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"lambda_1\", \"lambda_2\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combininig_lambda_ssk.to_csv(\"combininig_lambda_ssk.csv\")\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"earn\")\n",
    "    combininig_lambda_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combininig_lambda_ssk.to_csv(\"combininig_lambda_ssk.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"acq\")\n",
    "    combininig_lambda_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combininig_lambda_ssk.to_csv(\"combininig_lambda_ssk.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"crude\")\n",
    "    combininig_lambda_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combininig_lambda_ssk.to_csv(\"combininig_lambda_ssk.csv\", mode='a', header=False)\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"corn\")\n",
    "    combininig_lambda_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combininig_lambda_ssk.to_csv(\"combininig_lambda_ssk.csv\", mode='a', header=False)\n",
    "\n",
    "evaluation_for_combining_lambda_ssk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292615c",
   "metadata": {},
   "source": [
    "## ZANEMARITI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2070e10",
   "metadata": {},
   "source": [
    "Ako treniram modele s punim člancima, onda radi presporo tako da sam odlučila iz svakog članka izdvojiti n = 50 najčešćih riječi i onda po njima uspoređivati članke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8b879be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def return_n_most_common_words(row, n):\n",
    "    words_in_row = row['BODY'].split(\" \")\n",
    "    count = Counter()\n",
    "    for word in words_in_row:\n",
    "        if len(word) > 3:\n",
    "            count[word] += 1\n",
    "    lista = np.array([])\n",
    "    for (element, _) in count.most_common(n):\n",
    "        lista = np.append(lista, element)\n",
    "    #row['MOST_COMMON'] = lista\n",
    "    #print(lista)\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b84076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICS</th>\n",
       "      <th>BODY</th>\n",
       "      <th>MOST_COMMON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>champion products inc said its board of direct...</td>\n",
       "      <td>[said, board, stock, shares, shareholders, apr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>shr    cts vs      dlrs     net         vs    ...</td>\n",
       "      <td>[dlrs, assets, deposits, loans, note, availabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>ohio mattress co said its first quarter  endin...</td>\n",
       "      <td>[said, quarter, first, acquisitions, dlrs, sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>oper shr loss two cts vs profit seven cts     ...</td>\n",
       "      <td>[profit, oper, loss, revs, shrs, mths, seven, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>shr one dlr vs    cts     net      mln vs     ...</td>\n",
       "      <td>[revs, dlrs, nine, mths, billion, reuter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>shr loss nine cts vs loss    cts     net loss ...</td>\n",
       "      <td>[loss, dlrs, capitalized, costs, nine, revs, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13059</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>shr    cts vs    cts     shr diluted    cts vs...</td>\n",
       "      <td>[diluted, shrs, sales, nine, mths, dlrs, reuter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13060</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>shr    cts vs    cts     net      mln vs      ...</td>\n",
       "      <td>[dlrs, sales, shrs, nine, mths, oper, billion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13103</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>nine months ended august        group shr     ...</td>\n",
       "      <td>[billion, group, nine, months, ended, august, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13104</th>\n",
       "      <td>'earn'</td>\n",
       "      <td>third quarter ended august        group shr   ...</td>\n",
       "      <td>[billion, group, third, quarter, ended, august...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TOPICS                                               BODY  \\\n",
       "19     'earn'  champion products inc said its board of direct...   \n",
       "21     'earn'  shr    cts vs      dlrs     net         vs    ...   \n",
       "22     'earn'  ohio mattress co said its first quarter  endin...   \n",
       "24     'earn'  oper shr loss two cts vs profit seven cts     ...   \n",
       "25     'earn'  shr one dlr vs    cts     net      mln vs     ...   \n",
       "...       ...                                                ...   \n",
       "13058  'earn'  shr loss nine cts vs loss    cts     net loss ...   \n",
       "13059  'earn'  shr    cts vs    cts     shr diluted    cts vs...   \n",
       "13060  'earn'  shr    cts vs    cts     net      mln vs      ...   \n",
       "13103  'earn'  nine months ended august        group shr     ...   \n",
       "13104  'earn'  third quarter ended august        group shr   ...   \n",
       "\n",
       "                                             MOST_COMMON  \n",
       "19     [said, board, stock, shares, shareholders, apr...  \n",
       "21     [dlrs, assets, deposits, loans, note, availabl...  \n",
       "22     [said, quarter, first, acquisitions, dlrs, sea...  \n",
       "24     [profit, oper, loss, revs, shrs, mths, seven, ...  \n",
       "25             [revs, dlrs, nine, mths, billion, reuter]  \n",
       "...                                                  ...  \n",
       "13058  [loss, dlrs, capitalized, costs, nine, revs, s...  \n",
       "13059   [diluted, shrs, sales, nine, mths, dlrs, reuter]  \n",
       "13060  [dlrs, sales, shrs, nine, mths, oper, billion,...  \n",
       "13103  [billion, group, nine, months, ended, august, ...  \n",
       "13104  [billion, group, third, quarter, ended, august...  \n",
       "\n",
       "[3776 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earn_clanci['MOST_COMMON'] = earn_clanci.apply(lambda row: return_n_most_common_words(row, n=50), axis=1)\n",
    "acq_clanci['MOST_COMMON'] = acq_clanci.apply(lambda row: return_n_most_common_words(row, n=50), axis=1)\n",
    "crude_clanci['MOST_COMMON'] = crude_clanci.apply(lambda row: return_n_most_common_words(row, n=50), axis=1)\n",
    "corn_clanci['MOST_COMMON'] = corn_clanci.apply(lambda row: return_n_most_common_words(row, n=50), axis=1)\n",
    "earn_clanci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90779903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "## MOJ SSK -> radi za onaj mali uvodni primjer\n",
    "import itertools\n",
    "\n",
    "# SSK - string subsequence kernel\n",
    "def is_subsequence(subsequence, word):\n",
    "    iterator = iter(word)\n",
    "    if all(c in iterator for c in subsequence):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ssk_kernel(string1, string2, k=2, lambd=1):\n",
    "    stupci = []\n",
    "    tablica = {}\n",
    "\n",
    "    for word in [string1, string2]:\n",
    "        letters = list(word)\n",
    "        for combination in itertools.combinations(letters, k): # nalazi sve kombinacije slova u letters duljine k\n",
    "            s = ''.join(combination)\n",
    "            if s not in stupci:\n",
    "                stupci.append(s)\n",
    "    \n",
    "    #print(stupci)\n",
    "\n",
    "    for word in [string1, string2]:\n",
    "        tablica[word] = [0 for i in range(len(stupci))]\n",
    "        subsequence_index = 0\n",
    "        for stupac in stupci:\n",
    "            if is_subsequence(stupac, word):\n",
    "                cell_rez = 1\n",
    "                index_slova_rijeci = 0\n",
    "                for index_slova_stupca in range(len(stupac) - 1):\n",
    "                    cell_rez += word.index(stupac[index_slova_stupca+1], word.index(stupac[index_slova_stupca])+1)-word.index(stupac[index_slova_stupca])\n",
    "                tablica[word][subsequence_index] = pow(lambd, cell_rez)\n",
    "                #print(word, tablica[word])\n",
    "                # res += i.index(j[ki+1], i.index(j[ki])+1)-i.index(j[ki])\n",
    "            subsequence_index += 1\n",
    "    red_1 = np.array(tablica[string1])\n",
    "    red_2 = np.array(tablica[string2])\n",
    "    \n",
    "    rez_1 = np.sum(red_1*red_2.T)\n",
    "    rez_2 = np.sum(red_1*red_1.T)\n",
    "    rez_3 = np.sum(red_2*red_2.T)\n",
    "    rez = rez_1/pow(rez_2*rez_3, 0.5)\n",
    "    return rez\n",
    "\n",
    "print(ssk_kernel(\"cat\",\"car\", lambd=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGK - n-grams kernel\n",
    "# NGK is a linear kernel that returns a similarity score between documents\n",
    "# that are indexed by n-grams\n",
    "# vrijednost jezgrene funkcije\n",
    "def ngk(string1, string2):\n",
    "    def ngrams(string):\n",
    "        ngrams = set(())\n",
    "        for n in range(1, len(string)+1):\n",
    "            ngrams_helper = zip(*[string[i:] for i in range(n)])\n",
    "            for ngram in ngrams_helper:\n",
    "                ngrams.add(''.join(ngram))\n",
    "        #print(ngrams)\n",
    "        return ngrams\n",
    "    \n",
    "    ngrams_1 = ngrams(string1) # racuna ngrams za prvi dokument\n",
    "    ngrams_2 = ngrams(string2) # racuna ngrams za drugi dokument\n",
    "    \n",
    "    # usporeduje broj jednakih ngrams oba dokumenta\n",
    "    intercept_rez = ngrams_1.intersection(ngrams_2)\n",
    "    num_common = len(intercept_rez)\n",
    "    \n",
    "    rez = num_common/(len(ngrams_1)+len(ngrams_2))\n",
    "    rez = rez/0.5 #skaliranje\n",
    "    return rez\n",
    "\n",
    "def ngk_kernel(X1, X2):\n",
    "    kernel_matrix = np.zeros([len(X1), len(X2)])\n",
    "    for i in range(0, len(X1)):\n",
    "        for j in range(0, len(X2)):\n",
    "            kernel_matrix[i][j] = ngk(X1[i], X1[j])\n",
    "    return kernel_matrix\n",
    "\n",
    "print(ngk_kernel(\"car\",\"cat\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "52ff7c03b8138598d26f7db6d70fd0156ea4d6d9d9b70d008337a8032a894406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
