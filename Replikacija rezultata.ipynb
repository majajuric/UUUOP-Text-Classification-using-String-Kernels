{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce26ca48",
   "metadata": {},
   "source": [
    "Sveučilište u Zagrebu<br>\n",
    "Fakultet elektrotehnike i računarstva\n",
    "\n",
    "## Uvod u znanost o podacima\n",
    "\n",
    "# Replikacija rezultata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3015166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "049dcb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\majajuric\\anaconda3\\lib\\site-packages (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02b9fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3da48",
   "metadata": {},
   "source": [
    "Znanstveni rad opisuje novi način razvrstavanja članaka na temelju jezgrenih funkcija. Jezgrene funkcije predstavljaju umnožak u prostoru značajki. Jezgrene funkcije se koriste za klasifikaciju članaka jer je članke teško vektorizirati, odnosno pretvoriti u vektor značajki.\n",
    "\n",
    "Znanstveni rad opisuje implementaciju jezgre SSK koja navodno ima bolje performanse od standardnih jezgri NGK i WK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d398b",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a786e67",
   "metadata": {},
   "source": [
    "Jezgrene funkcije računaju sličnost između dva primjera. Sličnost se računa kao umnožak u prostoru značajki. Primjeri se samo implicitno preslikavaju u prostor značajki i tamo se množe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39dedaf",
   "metadata": {},
   "source": [
    "### WK kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b97a86",
   "metadata": {},
   "source": [
    "Standardni pristup klasifikaciji teksta preslikava tekst u visokodimenzionalni vektor u kojem svaki element vektora označava prisutnost ili nedostatak neke značajke. Ovakav pristup gubi svu informaciju o redoslijedu riječi te zadržava samo informaciju o frekvenciji pojavljivanja pojmova u dokumentu.\n",
    "\n",
    "Npr.\n",
    "s=\"science is organized knowledge\"\n",
    "t=\"wisdom is organized life\"\n",
    "\n",
    "feature vector = [\"science, is, organized, knowledge, wisdom, life]\n",
    "\n",
    "fi_1 = [1, 1, 1, 1, 0, 0]\n",
    "fi_2 = [0, 1, 1, 0, 1, 1]\n",
    "\n",
    "K(s, t) = [1, 1, 1, 1, 0, 0]*[0, 1, 1, 0, 1, 1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8fe097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kernels.wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f16adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk_kernel = lambda x, y: wk(x, y)\n",
    "\n",
    "def wkGmats(trainDocs, testDocs):\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = \"english\", input='content') \n",
    "\n",
    "    train_data_features = vectorizer.fit_transform(trainDocs)\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    tfidf = transformer.fit_transform(train_data_features)\n",
    "    tfidf = tfidf.toarray() \n",
    "\n",
    "    nTrainDocs = len(tfidf)\n",
    "    GmatTrain = np.ones((nTrainDocs,nTrainDocs))\n",
    "\n",
    "    for i in range( 0, nTrainDocs ):\n",
    "        for j in range(0,nTrainDocs):\n",
    "            GmatTrain[i][j] = np.dot(tfidf[i], tfidf[j])\n",
    "            \n",
    "    n_features_train = len(tfidf[0])\n",
    "    \n",
    "    train_data_features = vectorizer.transform(testDocs)\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    tfidfTest = transformer.fit_transform(train_data_features)\n",
    "    tfidfTest = tfidfTest.toarray() \n",
    "\n",
    "    nTestDocs = len(tfidfTest)\n",
    "    GmatTest = np.ones((nTestDocs,nTrainDocs))\n",
    "\n",
    "    for i in range( 0, nTestDocs ):\n",
    "        for j in range(0,nTrainDocs):\n",
    "            GmatTest[i][j] = np.dot(tfidfTest[i], tfidf[j])\n",
    "    \n",
    "    return GmatTrain, GmatTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "357cac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting WK, creating bag of words...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14851129125610232"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wk_train_gram_mat, wk_test_gram_mat] = wkGmats([\"science is organized knowledge\"], [\"wisdom is organized life\"])\n",
    "\n",
    "wk.wk(\"science is organized knowledge\",\"wisdom is organized life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750371e6",
   "metadata": {},
   "source": [
    "### NGK kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a063caa",
   "metadata": {},
   "source": [
    "NGK jezgra koristi n-grams. N-grams daju n susjednih slova nekog stringa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "529ef40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38207551689619024\n",
      "0.24129913647238913\n"
     ]
    }
   ],
   "source": [
    "import kernels.ngk\n",
    "\n",
    "doc1, doc2 = \"science is organized knowledge\", \"wisdom is organized life\"\n",
    "print(ngk.ngk(doc1, doc2))\n",
    "print(ngk.ngk(doc1, doc2, n=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5260b",
   "metadata": {},
   "source": [
    "### SSK kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e2d68",
   "metadata": {},
   "source": [
    "Cijeli dokument se promatra kao jedan dugačak sequence. Prostor značajki u ovom slučaju je set svih ne nužno susjednih substringova od k simbola. Dva članka su to sličniji što imaju više zajedničkih takvih substringova.\n",
    "\n",
    "Sličnost se računa u ovisnosti o lambdi koja mjeri težinu u ovisnosti u duljini i uzastopnosti charactera svakog subsequenca iz jednog dokumenta u drugom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec9ab86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kernels.ssk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a737b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssk_compute_train_gram(docs, kernel=None):\n",
    "    n = len(docs)\n",
    "    gram = np.ones((n, n))\n",
    "    print(\"SSK compute train gram...\")\n",
    "    for x in range(n):\n",
    "        #print('{0:.2f}%'.format(x / n * 100))\n",
    "        for y in range(x + 1, n):\n",
    "            gram[x, y] = kernel(docs[x], docs[y])\n",
    "            gram[y, x] = gram[x, y]\n",
    "    return gram  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3670d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssk_compute_test_gram(test, train, kernel=None):\n",
    "    gram = np.zeros((len(test), len(train)))\n",
    "    print(\"SSK compute test gram...\")\n",
    "    for x in range(len(test)):\n",
    "        #print('{0:.2f}%'.format(x / len(test) * 100))\n",
    "        for y in range(len(train)):\n",
    "            gram[x, y] = kernel(test[x], train[y])\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a97876a",
   "metadata": {},
   "source": [
    "### Priprema podataka\n",
    "U članku piše da su sve riječi u body-jima svih članaka pretvorene u lowercase. Također, uklonjene su sve *stopwords*, a interpunkcijski znakovi zamijenjeni su razmacima. Također, bitni su nam samo stupci TOPICS i BODY tako da ostale možemo izbaciti.\n",
    "\n",
    "U članku piše da su zadržali samo stem riječi. Pokušala sam to napraviti, ali javlja neku grešku s nltk --> pokušati opet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7351c0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICS</th>\n",
       "      <th>BODY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cocoa']</td>\n",
       "      <td>showers continued throughout week bahia cocoa zone alleviating drought since early january improving prospects coming temporao although normal humidity levels restored comissaria smith said weekly review dry period means temporao late year arrivals week ended february bags kilos making cumulative total season mln stage last year seems cocoa delivered earlier consignment included arrivals figures comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimates around mln bags sales standing almost mln hundred thousand bags still hands farmers middlemen exporters processors doubts much cocoa would fit export shippers experiencing dificulties obtaining bahia superior certificates view lower quality recent weeks farmers sold good part cocoa held consignment comissaria smith said spot bean prices rose cruzados per arroba kilos bean shippers reluctant offer nearby shipment limited sales booked march shipment dlrs per tonne ports named new crop sales also light open ports june july going dlrs dlrs new york july aug sept dlrs per tonne fob routine sales butter made march april sold dlrs april may butter went times new york may june july dlrs aug sept dlrs times new york sept oct dec dlrs times new york dec comissaria smith said destinations u covertible currency areas uruguay open ports cake sales registered dlrs march april dlrs may dlrs aug times new york dec oct dec buyers u argentina uruguay convertible currency areas liquor sales limited march april selling dlrs june july dlrs times new york july aug sept dlrs times new york sept oct dec times new york dec comissaria smith said total bahia sales currently estimated mln bags crop mln bags crop final figures period february expected published brazilian cocoa trade commission carnival ends midday february reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['grain', 'wheat', 'corn', 'barley', 'oat', 'sorghum']</td>\n",
       "      <td>u agriculture department reported farmer owned reserve national five day average price february follows dlrs bu sorghum cwt natl loan release call avge rate x level price price wheat iv v vi corn iv v x rates natl loan release call avge rate x level price price oats v barley n iv v sorghum iv v reserves ii iii matured level iv reflects grain entered oct feedgrain july wheat level v wheat barley corn sorghum level vi covers wheat entered january x rates dlrs per cwt lbs n available reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['veg-oil', 'linseed', 'lin-oil', 'soy-oil', 'sun-oil', 'soybean', 'oilseed', 'corn', 'sunseed', 'grain', 'sorghum', 'wheat']</td>\n",
       "      <td>argentine grain board figures show crop registrations grains oilseeds products february thousands tonnes showing future shipments month total total february brackets bread wheat prev feb march total maize mar total nil sorghum nil nil oilseed export registrations sunflowerseed total soybean may total nil board also detailed export registrations subproducts follows subproducts wheat prev feb march apr total linseed prev feb mar apr total soybean prev feb mar nil apr nil may total sunflowerseed prev feb mar apr total vegetable oil registrations sunoil prev feb mar apr may nil jun total linoil prev feb mar apr total soybean oil prev feb mar nil apr may jun jul total reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['earn']</td>\n",
       "      <td>champion products inc said board directors approved two one stock split common shares shareholders record april company also said board voted recommend shareholders annual meeting april increase authorized capital stock five mln mln shares reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['acq']</td>\n",
       "      <td>computer terminal systems inc said completed sale shares common stock warrants acquire additional one mln shares lt sedio n v lugano switzerland dlrs company said warrants exercisable five years purchase price dlrs per share computer terminal said sedio also right buy additional shares increase total holdings pct computer terminals outstanding common stock certain circumstances involving change control company company said conditions occur warrants would exercisable price equal pct common stocks market price time exceed dlrs per share computer terminal also said sold technolgy rights dot matrix impact technology including future improvements lt woodco inc houston tex dlrs said would continue exclusive worldwide licensee technology woodco company said moves part reorganization plan would help pay current operation costs ensure product delivery computer terminal makes computer generated labels forms tags ticket printers terminals reuter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                          TOPICS  \\\n",
       "0  ['cocoa']                                                                                                                       \n",
       "1  ['grain', 'wheat', 'corn', 'barley', 'oat', 'sorghum']                                                                          \n",
       "2  ['veg-oil', 'linseed', 'lin-oil', 'soy-oil', 'sun-oil', 'soybean', 'oilseed', 'corn', 'sunseed', 'grain', 'sorghum', 'wheat']   \n",
       "3  ['earn']                                                                                                                        \n",
       "4  ['acq']                                                                                                                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          BODY  \n",
       "0  showers continued throughout week bahia cocoa zone alleviating drought since early january improving prospects coming temporao although normal humidity levels restored comissaria smith said weekly review dry period means temporao late year arrivals week ended february bags kilos making cumulative total season mln stage last year seems cocoa delivered earlier consignment included arrivals figures comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimates around mln bags sales standing almost mln hundred thousand bags still hands farmers middlemen exporters processors doubts much cocoa would fit export shippers experiencing dificulties obtaining bahia superior certificates view lower quality recent weeks farmers sold good part cocoa held consignment comissaria smith said spot bean prices rose cruzados per arroba kilos bean shippers reluctant offer nearby shipment limited sales booked march shipment dlrs per tonne ports named new crop sales also light open ports june july going dlrs dlrs new york july aug sept dlrs per tonne fob routine sales butter made march april sold dlrs april may butter went times new york may june july dlrs aug sept dlrs times new york sept oct dec dlrs times new york dec comissaria smith said destinations u covertible currency areas uruguay open ports cake sales registered dlrs march april dlrs may dlrs aug times new york dec oct dec buyers u argentina uruguay convertible currency areas liquor sales limited march april selling dlrs june july dlrs times new york july aug sept dlrs times new york sept oct dec times new york dec comissaria smith said total bahia sales currently estimated mln bags crop mln bags crop final figures period february expected published brazilian cocoa trade commission carnival ends midday february reuter   \n",
       "1  u agriculture department reported farmer owned reserve national five day average price february follows dlrs bu sorghum cwt natl loan release call avge rate x level price price wheat iv v vi corn iv v x rates natl loan release call avge rate x level price price oats v barley n iv v sorghum iv v reserves ii iii matured level iv reflects grain entered oct feedgrain july wheat level v wheat barley corn sorghum level vi covers wheat entered january x rates dlrs per cwt lbs n available reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2  argentine grain board figures show crop registrations grains oilseeds products february thousands tonnes showing future shipments month total total february brackets bread wheat prev feb march total maize mar total nil sorghum nil nil oilseed export registrations sunflowerseed total soybean may total nil board also detailed export registrations subproducts follows subproducts wheat prev feb march apr total linseed prev feb mar apr total soybean prev feb mar nil apr nil may total sunflowerseed prev feb mar apr total vegetable oil registrations sunoil prev feb mar apr may nil jun total linoil prev feb mar apr total soybean oil prev feb mar nil apr may jun jul total reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3  champion products inc said board directors approved two one stock split common shares shareholders record april company also said board voted recommend shareholders annual meeting april increase authorized capital stock five mln mln shares reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4  computer terminal systems inc said completed sale shares common stock warrants acquire additional one mln shares lt sedio n v lugano switzerland dlrs company said warrants exercisable five years purchase price dlrs per share computer terminal said sedio also right buy additional shares increase total holdings pct computer terminals outstanding common stock certain circumstances involving change control company company said conditions occur warrants would exercisable price equal pct common stocks market price time exceed dlrs per share computer terminal also said sold technolgy rights dot matrix impact technology including future improvements lt woodco inc houston tex dlrs said would continue exclusive worldwide licensee technology woodco company said moves part reorganization plan would help pay current operation costs ensure product delivery computer terminal makes computer generated labels forms tags ticket printers terminals reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clanci_stripped = pd.read_csv(\"my_data/data/clanci_stripped.csv\", index_col = 0)\n",
    "clanci_stripped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176c772",
   "metadata": {},
   "source": [
    "Razdvajamo BODY-je po TOPICS-ima tako da u TOPICS nije lista nego samo jedna vrijednost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "393bdfb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clanci_stripped = clanci_stripped.loc[:, [\"TOPICS\", \"BODY\"]]\n",
    "clanci_stripped = clanci_stripped.loc[clanci_stripped.TOPICS.notnull(), :]\n",
    "#print(clanci.head(n=10))\n",
    "\n",
    "mapa = {'TOPICS': [], 'BODY': []}\n",
    "for index, row in clanci_stripped.iterrows():\n",
    "    topics = row.TOPICS.split(\",\")\n",
    "    for topic in topics:\n",
    "        topic = topic.replace('[', \"\")\n",
    "        topic = topic.replace(\"]\", \"\")\n",
    "        if mapa['TOPICS'] is None:\n",
    "            mapa['TOPICS'] = [topic]\n",
    "        else:\n",
    "            mapa['TOPICS'].append(topic)\n",
    "            \n",
    "        body = row.BODY\n",
    "        unfiltered_body = \"\"\n",
    "        for word in body.split(\" \"):\n",
    "            if len(word) > 3:\n",
    "                unfiltered_body+=word\n",
    "                unfiltered_body+=\" \"\n",
    "        filtered_body = \"\"\n",
    "        for character in unfiltered_body:\n",
    "            if (character.isalnum()) or (character == ' '):\n",
    "                filtered_body += character\n",
    "        filtered_body = filtered_body.replace('[^a-zA-Z]', \" \")\n",
    "        filtered_body = filtered_body.replace(' [ ]+', ' ')\n",
    "        \n",
    "        if mapa['BODY'] is None:\n",
    "            mapa['BODY'] = [filtered_body]\n",
    "        else:\n",
    "            mapa['BODY'].append(filtered_body)\n",
    "            \n",
    "dataframe = pd.DataFrame(mapa)\n",
    "dataframe.to_csv(\"my_data/data/clanci_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37ccacb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICS</th>\n",
       "      <th>BODY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'cocoa'</td>\n",
       "      <td>showers continued throughout week bahia cocoa zone alleviating drought since early january improving prospects coming temporao although normal humidity levels restored comissaria smith said weekly review period means temporao late year arrivals week ended february bags kilos making cumulative total season stage last year seems cocoa delivered earlier consignment included arrivals figures comissaria smith said still doubt much crop cocoa still available harvesting practically come total bahia crop estimates around bags sales standing almost hundred thousand bags still hands farmers middlemen exporters processors doubts much cocoa would export shippers experiencing dificulties obtaining bahia superior certificates view lower quality recent weeks farmers sold good part cocoa held consignment comissaria smith said spot bean prices rose cruzados arroba kilos bean shippers reluctant offer nearby shipment limited sales booked march shipment dlrs tonne ports named crop sales also light open ports june july going dlrs dlrs york july sept dlrs tonne routine sales butter made march april sold dlrs april butter went times york june july dlrs sept dlrs times york sept dlrs times york comissaria smith said destinations covertible currency areas uruguay open ports cake sales registered dlrs march april dlrs dlrs times york buyers argentina uruguay convertible currency areas liquor sales limited march april selling dlrs june july dlrs times york july sept dlrs times york sept times york comissaria smith said total bahia sales currently estimated bags crop bags crop final figures period february expected published brazilian cocoa trade commission carnival ends midday february reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'grain'</td>\n",
       "      <td>agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'wheat'</td>\n",
       "      <td>agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'corn'</td>\n",
       "      <td>agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'barley'</td>\n",
       "      <td>agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TOPICS  \\\n",
       "0  'cocoa'     \n",
       "1  'grain'     \n",
       "2   'wheat'    \n",
       "3   'corn'     \n",
       "4   'barley'   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              BODY  \n",
       "0  showers continued throughout week bahia cocoa zone alleviating drought since early january improving prospects coming temporao although normal humidity levels restored comissaria smith said weekly review period means temporao late year arrivals week ended february bags kilos making cumulative total season stage last year seems cocoa delivered earlier consignment included arrivals figures comissaria smith said still doubt much crop cocoa still available harvesting practically come total bahia crop estimates around bags sales standing almost hundred thousand bags still hands farmers middlemen exporters processors doubts much cocoa would export shippers experiencing dificulties obtaining bahia superior certificates view lower quality recent weeks farmers sold good part cocoa held consignment comissaria smith said spot bean prices rose cruzados arroba kilos bean shippers reluctant offer nearby shipment limited sales booked march shipment dlrs tonne ports named crop sales also light open ports june july going dlrs dlrs york july sept dlrs tonne routine sales butter made march april sold dlrs april butter went times york june july dlrs sept dlrs times york sept dlrs times york comissaria smith said destinations covertible currency areas uruguay open ports cake sales registered dlrs march april dlrs dlrs times york buyers argentina uruguay convertible currency areas liquor sales limited march april selling dlrs june july dlrs times york july sept dlrs times york sept times york comissaria smith said total bahia sales currently estimated bags crop bags crop final figures period february expected published brazilian cocoa trade commission carnival ends midday february reuter   \n",
       "1  agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "2  agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "3  agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "4  agriculture department reported farmer owned reserve national five average price february follows dlrs sorghum natl loan release call avge rate level price price wheat corn rates natl loan release call avge rate level price price oats barley sorghum reserves matured level reflects grain entered feedgrain july wheat level wheat barley corn sorghum level covers wheat entered january rates dlrs available reuter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca78e2",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5bf472",
   "metadata": {},
   "source": [
    "Ciljevi eksperimenata su:\n",
    "- proučavati utjecaj promjene parametara k(duljina) i $\\lambda$(težina)\n",
    "- uočiti prednosti kombiniranja različitih jezgri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aaf77b",
   "metadata": {},
   "source": [
    "### Podjela podataka u train i test skup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6d1c8",
   "metadata": {},
   "source": [
    "Eksperimenti su provedeni samo na dijelu Reuters seta. U članku piše da je subset bio veličine 470 dokumenata, od čega je 380 bilo korišteno za treniranje, a 90 za ispitivanje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bad638",
   "metadata": {},
   "source": [
    "U eksperimentu su odabrane kategorije \"earn\", \"acq\", \"crude\" i \"corn\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1fca2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "earn_clanci = dataframe[dataframe.TOPICS.str.contains(\"earn\")]\n",
    "acq_clanci = dataframe[dataframe.TOPICS.str.contains(\"acq\")]\n",
    "crude_clanci = dataframe[dataframe.TOPICS.str.contains(\"crude\")]\n",
    "corn_clanci = dataframe[dataframe.TOPICS.str.contains(\"corn\")]\n",
    "\n",
    "clanci = [earn_clanci, acq_clanci, crude_clanci, corn_clanci]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1e278",
   "metadata": {},
   "source": [
    "Navedeno je da je broj članaka za pojedinu kategoriju za učenje (ispitivanje) sljedeći:\n",
    "1. earn 152 (40)\n",
    "2. acquisition 114 (25)\n",
    "3. crude 76 (15)\n",
    "4. corn 38 (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f154e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def give_train_test_split(give_train):\n",
    "    [earn_tr, earn_te] = train_test_split(earn_clanci, train_size=152/len(earn_clanci), test_size=40/len(earn_clanci))\n",
    "    [acq_tr, acq_te] = train_test_split(acq_clanci, train_size=114/len(acq_clanci), test_size=25/len(acq_clanci))\n",
    "    [crude_tr, crude_te] = train_test_split(crude_clanci, train_size=76/len(crude_clanci), test_size=15/len(crude_clanci))\n",
    "    [corn_tr, corn_te] = train_test_split(corn_clanci, train_size=38/len(corn_clanci), test_size=10/len(corn_clanci))\n",
    "\n",
    "    y_train = []\n",
    "    y_train.extend(['earn' for i in range(0, len(earn_tr))])\n",
    "    y_train.extend(['acq' for i in range(0, len(acq_tr))])\n",
    "    y_train.extend(['crude' for i in range(0, len(crude_tr))])\n",
    "    y_train.extend(['corn' for i in range(0, len(corn_tr))])\n",
    "    y_train = np.array(y_train)\n",
    "    #print(y_train)\n",
    "\n",
    "    y_test = []\n",
    "    y_test.extend(['earn' for i in range(0, len(earn_te))])\n",
    "    y_test.extend(['acq' for i in range(0, len(acq_te))])\n",
    "    y_test.extend(['crude' for i in range(0, len(crude_te))])\n",
    "    y_test.extend(['corn' for i in range(0, len(corn_te))])\n",
    "    y_test = np.array(y_test)\n",
    "    #print(y_test)\n",
    "\n",
    "    clanci_test = [earn_te, acq_te, crude_te, corn_te]\n",
    "    clanci_test = pd.concat(clanci_test)\n",
    "    clanci_train = [earn_tr, acq_tr, crude_tr, corn_tr]\n",
    "    clanci_train = pd.concat(clanci_train)\n",
    "    \n",
    "    earn_test=[]\n",
    "    acq_test=[]\n",
    "    crude_test=[]\n",
    "    corn_test=[]\n",
    "    for index, row in earn_te.iterrows():\n",
    "        earn_test.append(row['BODY'])\n",
    "    for index, row in acq_te.iterrows():\n",
    "        acq_test.append(row['BODY'])\n",
    "    for index, row in crude_te.iterrows():\n",
    "        crude_test.append(row['BODY'])\n",
    "    for index, row in corn_te.iterrows():\n",
    "        corn_test.append(row['BODY'])\n",
    "    \n",
    "    treniranje_parovi = []\n",
    "    treniranje = []\n",
    "    i = 0\n",
    "    for index, row in clanci_train.iterrows():\n",
    "        par = []\n",
    "        par = [row['BODY'], y_train[i]]\n",
    "        treniranje.append(row['BODY'])\n",
    "        treniranje_parovi.append(par)\n",
    "\n",
    "    testiranje = []\n",
    "    for index, row in clanci_test.iterrows():\n",
    "        testiranje.append(row['BODY'])\n",
    "    \n",
    "    if give_train:\n",
    "        return [treniranje, y_train]\n",
    "    else:\n",
    "        return [earn_test, acq_test, crude_test, corn_test, testiranje, y_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee86bf1",
   "metadata": {},
   "source": [
    "### Effectiveness of Varying Sequence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc05ef",
   "metadata": {},
   "source": [
    "U ovom dijelu promatramo kako parametar duljine subsequenca, k, utječe na točnost modela. Za svaku vrijednost k, eksperiment je proveden 10 puta i onda su dobivene vrijednosti mean i sd. Lambda je postavljen na 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc49a6",
   "metadata": {},
   "source": [
    "Kako je računanje za SSK jako sporo, kod njega sam izostavila provođenje eksperimenta 10 puta pa se on provodi samo par puta. Za NGK i WK se provodi 10 puta i vrijednosti evaluacije rezultata su uprosječene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e654a",
   "metadata": {},
   "source": [
    "Stvorimo listu u koju stavljamo [category, ime ljuske, length, f1_mean, f1_std, precision_mean, precision_std, recall_mean, recall_std]. Od te liste kasnije stvorimo dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f4615",
   "metadata": {},
   "source": [
    "#### Evaluacija SSK jezgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a9cda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ispisuje tablicu kao sto je u radu\n",
    "def ssk_evaluation(category, k_range=[5], lambd_range=[0.5], filename=\"my_data/results/no_file.csv\"):\n",
    "    rezultat_lista = []\n",
    "\n",
    "    print(\"Starting SSK evaluation for {}...\".format(category))\n",
    "    for k in k_range:\n",
    "        for lambd in lambd_range:\n",
    "            print(\"\\tk = {}, lambda = {}\".format(k, lambd))\n",
    "            lista_u_ovom_koraku = []\n",
    "            lista_u_ovom_koraku.append(category)\n",
    "            lista_u_ovom_koraku.append(\"SSK\")\n",
    "            f1 = []\n",
    "            precision = []\n",
    "            recall = []\n",
    "            for j in range(0, 10):\n",
    "                [trening, y_trening] = give_train_test_split(True)\n",
    "\n",
    "                #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "                treniranje = []\n",
    "                y_train = []\n",
    "                for i in range(0, 60):\n",
    "                    r = random.randint(0, len(trening)-1)\n",
    "                    treniranje.append(trening[r])\n",
    "                    y_train.append(y_trening[r])\n",
    "\n",
    "                ssk_kernel = lambda x, y: ssk.ssk(x, y, k, lambd)\n",
    "                train_gram = ssk_compute_train_gram(treniranje, kernel=ssk_kernel)\n",
    "                [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "                if(category==\"earn\"):\n",
    "                    X_test = earn_test\n",
    "                    y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "                elif category == \"acq\":\n",
    "                    X_test = acq_test\n",
    "                    y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "                elif category == \"crude\":\n",
    "                    X_test = crude_test\n",
    "                    y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "                else:\n",
    "                    X_test = corn_test\n",
    "                    y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "\n",
    "                test_gram = ssk_compute_test_gram(X_test, treniranje, kernel=ssk_kernel)\n",
    "                clf = SVC(kernel='precomputed')\n",
    "                clf.fit(train_gram, y_train)\n",
    "                # predikcija\n",
    "                y_pred = clf.predict(test_gram)\n",
    "                f1.append(f1_score(y_true, y_pred, average='micro'))\n",
    "                precision.append(precision_score(y_true, y_pred, average='micro'))\n",
    "                recall.append(recall_score(y_true, y_pred, average='micro'))\n",
    "            # kraj j petlje\n",
    "        \n",
    "            if len(k_range) == 1:\n",
    "                lista_u_ovom_koraku.append(lambd)\n",
    "            else:\n",
    "                lista_u_ovom_koraku.append(k)\n",
    "            \n",
    "            lista_u_ovom_koraku.append(round(np.mean(f1), 4))\n",
    "            lista_u_ovom_koraku.append(round(np.std(f1), 4))\n",
    "            lista_u_ovom_koraku.append(round(np.mean(precision), 4))\n",
    "            lista_u_ovom_koraku.append(round(np.std(precision), 4))\n",
    "            lista_u_ovom_koraku.append(round(np.mean(recall), 4))\n",
    "            lista_u_ovom_koraku.append(round(np.std(recall), 4))\n",
    "        \n",
    "            rez = pd.DataFrame([lista_u_ovom_koraku])\n",
    "            rez.to_csv(filename, mode='a', header=False)       \n",
    "        \n",
    "            rezultat_lista.append(lista_u_ovom_koraku)\n",
    "    print(\"End of SSK evaluation\\n\")\n",
    "    return rezultat_lista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014de0d",
   "metadata": {},
   "source": [
    "#### Evaluacija NGK jezgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76f093fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngk_evaluation(category, k_range=[5], filename=\"my_data/results/no_file.csv\"):\n",
    "    rezultat_lista = []\n",
    "    \n",
    "    print(\"Starting NGK evaluation for {}...\".format(category))\n",
    "    for k in k_range:\n",
    "        lista_u_ovom_koraku = []\n",
    "        lista_u_ovom_koraku.append(category)\n",
    "        lista_u_ovom_koraku.append(\"NGK\")\n",
    "        print(\"\\t k =\", k)\n",
    "        f1 = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for i in range(0, 10):\n",
    "            #treniranje jezgre\n",
    "            [trening, y_trening] = give_train_test_split(True)\n",
    "            \n",
    "            #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "            treniranje = []\n",
    "            y_train = []\n",
    "            for i in range(0, 60):\n",
    "                r = random.randint(0, len(trening)-1)\n",
    "                treniranje.append(trening[r])\n",
    "                y_train.append(y_trening[r])\n",
    "        \n",
    "            [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "            if(category==\"earn\"):\n",
    "                X_test = earn_test\n",
    "                y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "            elif category == \"acq\":\n",
    "                X_test = acq_test\n",
    "                y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "            elif category == \"crude\":\n",
    "                X_test = crude_test\n",
    "                y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "            else:\n",
    "                X_test = corn_test\n",
    "                y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "\n",
    "            train_gram, test_gram = ngk.ngkGmats(treniranje, X_test, n=k)\n",
    "            clf = SVC(kernel='precomputed')\n",
    "            clf.fit(train_gram, y_train)\n",
    "            y_pred = clf.predict(test_gram)\n",
    "            f1.append(round(f1_score(y_true, y_pred, average='micro'), 3))\n",
    "            precision.append(round(precision_score(y_true, y_pred, average='micro'), 3))\n",
    "            recall.append(round(recall_score(y_true, y_pred, average='micro'), 3))\n",
    "        \n",
    "        if len(k_range) == 1:\n",
    "            lista_u_ovom_koraku.append(0)\n",
    "        else:\n",
    "            lista_u_ovom_koraku.append(k)\n",
    "        lista_u_ovom_koraku.append(round(np.mean(f1), 4))\n",
    "        lista_u_ovom_koraku.append(round(np.std(f1), 4))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(precision), 4))\n",
    "        lista_u_ovom_koraku.append(round(np.std(precision), 4))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(recall), 4))\n",
    "        lista_u_ovom_koraku.append(round(np.std(recall), 4))\n",
    "        \n",
    "        rez = pd.DataFrame([lista_u_ovom_koraku])\n",
    "        rez.to_csv(filename, mode='a', header=False)      \n",
    "        \n",
    "        rezultat_lista.append(lista_u_ovom_koraku)\n",
    "        #print(lista_u_ovom_koraku)\n",
    "    print(\"End of NGK evaluation\\n\")\n",
    "    return rezultat_lista\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c260b4",
   "metadata": {},
   "source": [
    "#### Evaluacija WK jezgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3300614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wk_evaluation(category, filename=\"my_data/results/no_file.csv\"):\n",
    "    rezultat_lista = []\n",
    "    rezultat_lista.append(category)\n",
    "    rezultat_lista.append(\"WK\")\n",
    "\n",
    "    f1 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    print(\"Starting WK evaluation for {}...\".format(category))\n",
    "    for i in range(0, 10):\n",
    "        #print(category, i)\n",
    "        #treniranje jezgre\n",
    "        \n",
    "        [trening, y_trening] = give_train_test_split(True)\n",
    "            \n",
    "        #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "        treniranje = []\n",
    "        y_train = []\n",
    "        for i in range(0, 60):\n",
    "            r = random.randint(0, len(trening)-1)\n",
    "            treniranje.append(trening[r])\n",
    "            y_train.append(y_trening[r])\n",
    "        \n",
    "        [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "        if(category==\"earn\"):\n",
    "            X_test = earn_test\n",
    "            y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "        elif category == \"acq\":\n",
    "            X_test = acq_test\n",
    "            y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "        elif category == \"crude\":\n",
    "            X_test = crude_test\n",
    "            y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "        else:\n",
    "            X_test = corn_test\n",
    "            y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "\n",
    "        wk_kernel = lambda x, y: wk(x, y)\n",
    "        [wk_train_gram, wk_test_gram] = wkGmats(treniranje, X_test)\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(wk_train_gram, y_train)\n",
    "        y_pred = clf.predict(wk_test_gram)\n",
    "        f1.append(round(f1_score(y_true, y_pred, average='micro'), 3))\n",
    "        precision.append(round(precision_score(y_true, y_pred, average='micro'), 3))\n",
    "        recall.append(round(recall_score(y_true, y_pred, average='micro'), 3))\n",
    "        \n",
    "    rezultat_lista.append(0)\n",
    "    rezultat_lista.append(round(np.mean(f1), 4))\n",
    "    rezultat_lista.append(round(np.std(f1), 4))\n",
    "    rezultat_lista.append(round(np.mean(precision), 4))\n",
    "    rezultat_lista.append(round(np.std(precision), 4))\n",
    "    rezultat_lista.append(round(np.mean(recall), 4))\n",
    "    rezultat_lista.append(round(np.std(recall), 4))\n",
    "    \n",
    "    rez = pd.DataFrame([rezultat_lista])\n",
    "    rez.to_csv(filename, mode='a', header=False) \n",
    "    \n",
    "    print(\"End of WK evaluation\\n\")\n",
    "    return [rezultat_lista]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21779125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NGK evaluation for earn...\n",
      "\t k = 3\n",
      "\t k = 4\n",
      "\t k = 5\n",
      "\t k = 6\n",
      "\t k = 7\n",
      "\t k = 8\n",
      "\t k = 9\n",
      "\t k = 10\n",
      "End of NGK evaluation\n",
      "\n",
      "Starting SSK evaluation for earn...\n",
      "\tk = 3, lambda = 0.5\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "\tk = 4, lambda = 0.5\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "\tk = 5, lambda = 0.5\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "\tk = 6, lambda = 0.5\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "\tk = 7, lambda = 0.5\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "\tk = 8, lambda = 0.5\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n",
      "SSK compute test gram...\n",
      "SSK compute train gram...\n"
     ]
    }
   ],
   "source": [
    "# k_range=[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "def evaluation_for_varying_sequence_lengths():\n",
    "    filename = \"my_data/results/varying_sequence_length.csv\"\n",
    "    rezultat = []\n",
    "    varying_sequence_length = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"k\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_sequence_length.to_csv(filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"earn\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"earn\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"earn\", filename=filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"acq\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"acq\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"acq\", filename=filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"crude\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"crude\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"crude\", filename=filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"corn\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"corn\", k_range=[*range(3, 10+1, 1)], filename=filename)\n",
    "\n",
    "    rezultat = wk_evaluation(\"corn\", filename=filename)\n",
    "    \n",
    "evaluation_for_varying_sequence_lengths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888a079",
   "metadata": {},
   "source": [
    "#### Usporedba rezultata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fc0a6",
   "metadata": {},
   "source": [
    "Radi brzine izvođenja, nisam računala za sve k kao što je u znanstvenom radu. Ali se može uočiti da SSK najbolje radi za male i srednje velike k (otprilike 4-7). Parametar k može se postaviti unaprijed unakrsnom provjerom tako da maksimizira točnost (minimizira pogrešku) na skupu za provjeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_data/results/varying_sequence_length.csv\"\n",
    "rezultat = pd.read_csv(filename, index_col=[0, 1, 2, 3])\n",
    "display(rezultat)\n",
    "#pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ee940",
   "metadata": {},
   "source": [
    "### Effectiveness of Varying Weight Decay Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587c1e8",
   "metadata": {},
   "source": [
    "Sada ispitujemo model za promjenjive vrijednosti lambde. Parametar $\\lambda$ upravlja \"kažnjavanjem\" ne-susjednih substringova. Što su stringovi \"ne-susjedniji\" u člancima, to su više kažnjeni.\n",
    "\n",
    "Pozivamo iste funkcije kao i za Varying Sequence Length, ali ovaj puta predajemo niz lambdi (weight decay factor) za koje testiramo model.\n",
    "\n",
    "Za svaku vrijednost $\\lambda$, eksperiment je proveden 10 puta i onda su dobivene vrijednosti mean i sd. Parametar k je postavljen na 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rezultat_lista = []\n",
    "def evaluation_for_varying_weight_decay_factors():\n",
    "    filename = \"my_data/results/varying_weight_decay.csv\"\n",
    "    rezultat = []\n",
    "    varying_weight_decay = pd.DataFrame(rezultat, columns=[\"category\", \"ime ljuske\", \"lambda\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    varying_weight_decay.to_csv(filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"earn\", filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"earn\", lambd_range=[*range(0.01, 1, 0.2)], filename=filename)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"earn\", filename=filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"acq\", filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"acq\", lambd_range=[*range(0.01, 1, 0.2)], filename=filename)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"acq\", filename=filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"crude\", filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"crude\", lambd_range=[*range(0.01, 1, 0.2)], filename=filename)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"crude\", filename=filename)\n",
    "    \n",
    "    rezultat = ngk_evaluation(\"corn\", filename=filename)\n",
    "    \n",
    "    rezultat = ssk_evaluation(\"corn\", lambd_range=[*range(0.01, 1, 0.2)], filename=filename)\n",
    "    \n",
    "    rezultat = wk_evaluation(\"corn\", filename=filename)\n",
    "\n",
    "evaluation_for_varying_weight_decay_factors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2895bd",
   "metadata": {},
   "source": [
    "#### Usporedba rezultata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af345a6c",
   "metadata": {},
   "source": [
    "Moji rezultati se i ne poklapaju baš s onima danim u znanstvenom radu. Kod njih za sve kategorije osim 'corn' preciznost postiže vrhunac za veće vrijednosti lambde. Kod kategorije 'corn', koja postiže najveću preciznost za lambda=0.3, vidimo da povećanje lambde ne znači nužno i povećanje preciznosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3586a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_data/results/varying_weight_decay.csv\"\n",
    "rezultat = pd.read_csv(filename, index_col=[0, 1, 2, 3])\n",
    "display(rezultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2ccae",
   "metadata": {},
   "source": [
    "### Effectiveness of Combining Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974341f",
   "metadata": {},
   "source": [
    "Sljedeće se promatra ako kombinacija jezgri pomaže generalizaciji modela, odnosno je li model točniji na neviđenim podacima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd539ee",
   "metadata": {},
   "source": [
    "#### Combining NGK and SSK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3592c6",
   "metadata": {},
   "source": [
    "Ovdje smo kombinirali NGK i SSK jezgre. Koristili smo njihovu težinsku sumu tako da w_ng predstavlja utjecaj NGK, a w_sk utjecaj SSK u sumi. Parametri k i lambda postavljeni su na 5 i 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cafebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "lambd = 0.5\n",
    "def NGK_SSK_comb_evaluation(category, filename=\"my_data/results/no_file.csv\"):\n",
    "    rezultat_lista = []\n",
    "    w_ng_list = [1, 0.5, 0.8, 0.9] #[1, 0, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    w_sk_list = [0, 0.5, 0.2, 0.1] #[0, 1, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    print(\"Starting NGK_SSK_COMB evaluation...\")\n",
    "    for i in range(0, len(w_ng_list)):\n",
    "        w_ng = w_ng_list[i]\n",
    "        w_sk = w_sk_list[i]\n",
    "        lista_u_ovom_koraku = []\n",
    "        lista_u_ovom_koraku.append(category)\n",
    "        lista_u_ovom_koraku.append(w_ng)\n",
    "        lista_u_ovom_koraku.append(w_sk)\n",
    "        print(\"\\tw_ng={} w_sk={}\".format(w_ng, w_sk))\n",
    "        f1 = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for j in range(0, 3):\n",
    "            #treniranje jezgre\n",
    "            [trening, y_trening] = give_train_test_split(True)\n",
    "\n",
    "            #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "            treniranje = []\n",
    "            y_train = []\n",
    "            for it in range(0, 60):\n",
    "                r = random.randint(0, len(trening)-1)\n",
    "                treniranje.append(trening[r])\n",
    "                y_train.append(y_trening[r])\n",
    "\n",
    "            [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "            if(category==\"earn\"):\n",
    "                X_test = earn_test\n",
    "                y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "            elif category == \"acq\":\n",
    "                X_test = acq_test\n",
    "                y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "            elif category == \"crude\":\n",
    "                X_test = crude_test\n",
    "                y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "            else:\n",
    "                X_test = corn_test\n",
    "                y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "\n",
    "            ssk_kernel = lambda x, y: ssk.ssk(x, y, 5, 0.5)\n",
    "            ssk_train_gram = ssk_compute_train_gram(treniranje, kernel=ssk_kernel)\n",
    "            ssk_test_gram = ssk_compute_test_gram(X_test, treniranje, kernel=ssk_kernel)\n",
    "            ngk_train_gram, ngk_test_gram = ngk.ngkGmats(treniranje, X_test, n=5)\n",
    "\n",
    "            test_gram = ngk_test_gram*w_ng + ssk_test_gram*w_sk\n",
    "            train_gram = ngk_train_gram*w_ng + ssk_train_gram*w_sk\n",
    "\n",
    "            clf = SVC(kernel='precomputed')\n",
    "            clf.fit(train_gram, y_train)\n",
    "            y_pred = clf.predict(test_gram)\n",
    "            f1.append(f1_score(y_true, y_pred, average='micro'))\n",
    "            precision.append(precision_score(y_true, y_pred, average='micro'))\n",
    "            recall.append(recall_score(y_true, y_pred, average='micro'))\n",
    "        # kraj for j petlje\n",
    "        \n",
    "        lista_u_ovom_koraku.append(round(np.mean(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(recall), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(recall), 3))\n",
    "        \n",
    "        rez = pd.DataFrame([lista_u_ovom_koraku])\n",
    "        rez.to_csv(filename, mode='a', header=False)   \n",
    "        \n",
    "        #print(lista_u_ovom_koraku)\n",
    "        rezultat_lista.append(lista_u_ovom_koraku)\n",
    "        \n",
    "    print(\"Ending NGK_SSK_COMB evaluation\\n\")\n",
    "    return rezultat_lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4636a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_for_combining_ngk_and_ssk():\n",
    "    rezultat = []\n",
    "    filename = \"my_data/results/combining_ngk_and_ssk.csv\"\n",
    "    combining_ngk_and_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"w_ng\", \"w_sk\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combining_ngk_and_ssk.to_csv()\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"earn\", filename)\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"acq\", filename)\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"crude\", filename)\n",
    "    \n",
    "    rezultat = NGK_SSK_comb_evaluation(\"corn\", filename)\n",
    "\n",
    "evaluation_for_combining_ngk_and_ssk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb2a18",
   "metadata": {},
   "source": [
    "#### Usporedba rezultata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_data/results/combining_ngk_and_ssk.csv\"\n",
    "rezultat = pd.read_csv(filename, index_col=[0, 1, 2, 3])\n",
    "display(rezultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db0429",
   "metadata": {},
   "source": [
    "#### Combining SSK with different weight decay factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ab066",
   "metadata": {},
   "source": [
    "Sljedeće smo proučavali kombiniranje dvije SSK jezgre s različitim lambdama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4e7f5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSK_lambda_comb_evaluation(category, filename=\"my_data/results/no_file.csv\"):\n",
    "    rezultat_lista = []\n",
    "    lambda_1_list = [0.05, 0.5, 0.05]\n",
    "    lambda_2_list = [0.0, 0.0, 0.5]\n",
    "    print(\"Starting combining SSK with differend lambdas for {}...\".format(category))\n",
    "    for i in range(0, len(lambda_1_list)):\n",
    "        lambda1 = lambda_1_list[i]\n",
    "        lambda2 = lambda_2_list[i]\n",
    "        lista_u_ovom_koraku = []\n",
    "        lista_u_ovom_koraku.append(category)\n",
    "        lista_u_ovom_koraku.append(lambda1)\n",
    "        lista_u_ovom_koraku.append(lambda2)\n",
    "        print(\"\\tlambda_1={} lambda_2={}\".format(lambda1, lambda2))\n",
    "        f1 = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        #for j in range(0, 10):\n",
    "        #treniranje jezgre\n",
    "        [trening, y_trening] = give_train_test_split(True)\n",
    "            \n",
    "        #odabire podskup primjera za ucenje jer ih je inace previse i dugo traje\n",
    "        treniranje = []\n",
    "        y_train = []\n",
    "        for i in range(0, 60):\n",
    "            r = random.randint(0, len(trening)-1)\n",
    "            treniranje.append(trening[r])\n",
    "            y_train.append(y_trening[r])\n",
    "            \n",
    "        [earn_test, acq_test, crude_test, corn_test, _, _] = give_train_test_split(False)\n",
    "        if(category==\"earn\"):\n",
    "            X_test = earn_test\n",
    "            y_true = np.array(['earn' for i in range(0, len(earn_test))])\n",
    "        elif category == \"acq\":\n",
    "            X_test = acq_test\n",
    "            y_true = np.array(['acq' for i in range(0, len(acq_test))])\n",
    "        elif category == \"crude\":\n",
    "            X_test = crude_test\n",
    "            y_true = np.array(['crude' for i in range(0, len(crude_test))])\n",
    "        else:\n",
    "            X_test = corn_test\n",
    "            y_true = np.array(['corn' for i in range(0, len(corn_test))])\n",
    "        \n",
    "        SSKTrainGram = np.zeros((len(treniranje),len(treniranje)))\n",
    "        for m in range(0, len(treniranje)):\n",
    "            for n in range(0, len(treniranje)):\n",
    "                SSKTrainGram[m][n] = ssk.ssk(treniranje[m], treniranje[n], 5, lambda1) + ssk.ssk(treniranje[m], treniranje[n], 5, lambda2) \n",
    "        \n",
    "        SSKTestGram = np.zeros((len(X_test),len(X_test)))\n",
    "        for m in range(0, len(X_test)):\n",
    "            for n in range(0, len(X_test)):\n",
    "                SSKTrainGram[m][n] = ssk.ssk(X_test[m], X_test[n], 5, lambda1) + ssk.ssk(treniranje[m], treniranje[n], 5, lambda2)\n",
    "\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(SSKTrainGram, y_train)\n",
    "        y_pred = clf.predict(SSKTestGram)\n",
    "        f1.append(f1_score(y_true, y_pred, average='micro'))\n",
    "        precision.append(precision_score(y_true, y_pred, average='micro'))\n",
    "        recall.append(recall_score(y_true, y_pred, average='micro'))\n",
    "        # kraj j petlje\n",
    "        \n",
    "        lista_u_ovom_koraku.append(round(np.mean(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(f1), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(precision), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.mean(recall), 3))\n",
    "        lista_u_ovom_koraku.append(round(np.std(recall), 3))\n",
    "        \n",
    "        rez = pd.DataFrame([lista_u_ovom_koraku])\n",
    "        rez.to_csv(filename, mode='a', header=False)\n",
    "        \n",
    "        #print(lista_u_ovom_koraku)\n",
    "        rezultat_lista.append(lista_u_ovom_koraku)\n",
    "    \n",
    "    print(\"End of combining SSK with differend lambdas\\n\")\n",
    "    return rezultat_lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f8ea8f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combining SSK with differend lambdas for earn...\n",
      "\tlambda_1=0.05 lambda_2=0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     combining_lambda_ssk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw_ng\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw_sk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     20\u001b[0m     combining_lambda_ssk\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombining_lambda_ssk.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mevaluation_for_combining_lambda_ssk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36mevaluation_for_combining_lambda_ssk\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m combining_lambda_ssk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m combining_lambda_ssk\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombining_lambda_ssk.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m rezultat \u001b[38;5;241m=\u001b[39m \u001b[43mSSK_lambda_comb_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m combining_lambda_ssk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rezultat, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw_ng\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw_sk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      8\u001b[0m combining_lambda_ssk\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombining_lambda_ssk.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [183]\u001b[0m, in \u001b[0;36mSSK_lambda_comb_evaluation\u001b[1;34m(category)\u001b[0m\n\u001b[0;32m     51\u001b[0m         SSKTrainGram[m][n] \u001b[38;5;241m=\u001b[39m ssk\u001b[38;5;241m.\u001b[39mssk(X_test[m], X_test[n], \u001b[38;5;241m5\u001b[39m, lambda1) \u001b[38;5;241m+\u001b[39m ssk\u001b[38;5;241m.\u001b[39mssk(treniranje[m], treniranje[n], \u001b[38;5;241m5\u001b[39m, lambda2)\n\u001b[0;32m     53\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSSKTrainGram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(SSKTestGram)\n\u001b[0;32m     56\u001b[0m f1\u001b[38;5;241m.\u001b[39mappend(f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    797\u001b[0m         )\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 800\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    803\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    108\u001b[0m         allow_nan\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(X)\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m    112\u001b[0m     ):\n\u001b[0;32m    113\u001b[0m         type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfinity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_nan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN, infinity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m             msg_err\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    116\u001b[0m                 type_err, msg_dtype \u001b[38;5;28;01mif\u001b[39;00m msg_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    117\u001b[0m             )\n\u001b[0;32m    118\u001b[0m         )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "def evaluation_for_combining_lambda_ssk():\n",
    "    rezultat = []\n",
    "    filename = \"my_data/results/combining_lambda_ssk.csv\"\n",
    "    combining_lambda_ssk = pd.DataFrame(rezultat, columns=[\"category\", \"lambda_1\", \"lambda_2\", \"f1_mean\", \"f1_std\", \"precision_mean\", \"precision_std\", \"recall_mean\", \"recall_std\"])\n",
    "    combining_lambda_ssk.to_csv(filename)\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"earn\", filename)\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"acq\", filename)\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"crude\", filename)\n",
    "    \n",
    "    rezultat = SSK_lambda_comb_evaluation(\"corn\", filename)\n",
    "\n",
    "evaluation_for_combining_lambda_ssk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33e6f9",
   "metadata": {},
   "source": [
    "#### Usporedba rezultata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455e9dd",
   "metadata": {},
   "source": [
    "Ovaj dio iz nekog razloga baca grešku..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81568108",
   "metadata": {},
   "source": [
    "### Combining kernels of different lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94fe88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664e457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5112a02",
   "metadata": {},
   "source": [
    "#### Usporedba rezultata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cf435a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22bd8e07",
   "metadata": {},
   "source": [
    "### Moji pokušaji implementacija jezgri koji su bili krivi/prespori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90779903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "## MOJ SSK -> radi za onaj mali uvodni primjer\n",
    "import itertools\n",
    "\n",
    "# SSK - string subsequence kernel\n",
    "def is_subsequence(subsequence, word):\n",
    "    iterator = iter(word)\n",
    "    if all(c in iterator for c in subsequence):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ssk_kernel(string1, string2, k=2, lambd=1):\n",
    "    stupci = []\n",
    "    tablica = {}\n",
    "\n",
    "    for word in [string1, string2]:\n",
    "        letters = list(word)\n",
    "        for combination in itertools.combinations(letters, k): # nalazi sve kombinacije slova u letters duljine k\n",
    "            s = ''.join(combination)\n",
    "            if s not in stupci:\n",
    "                stupci.append(s)\n",
    "    \n",
    "    #print(stupci)\n",
    "\n",
    "    for word in [string1, string2]:\n",
    "        tablica[word] = [0 for i in range(len(stupci))]\n",
    "        subsequence_index = 0\n",
    "        for stupac in stupci:\n",
    "            if is_subsequence(stupac, word):\n",
    "                cell_rez = 1\n",
    "                index_slova_rijeci = 0\n",
    "                for index_slova_stupca in range(len(stupac) - 1):\n",
    "                    cell_rez += word.index(stupac[index_slova_stupca+1], word.index(stupac[index_slova_stupca])+1)-word.index(stupac[index_slova_stupca])\n",
    "                tablica[word][subsequence_index] = pow(lambd, cell_rez)\n",
    "                #print(word, tablica[word])\n",
    "                # res += i.index(j[ki+1], i.index(j[ki])+1)-i.index(j[ki])\n",
    "            subsequence_index += 1\n",
    "    red_1 = np.array(tablica[string1])\n",
    "    red_2 = np.array(tablica[string2])\n",
    "    \n",
    "    rez_1 = np.sum(red_1*red_2.T)\n",
    "    rez_2 = np.sum(red_1*red_1.T)\n",
    "    rez_3 = np.sum(red_2*red_2.T)\n",
    "    rez = rez_1/pow(rez_2*rez_3, 0.5)\n",
    "    return rez\n",
    "\n",
    "print(ssk_kernel(\"cat\",\"car\", lambd=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGK - n-grams kernel\n",
    "# NGK is a linear kernel that returns a similarity score between documents\n",
    "# that are indexed by n-grams\n",
    "# vrijednost jezgrene funkcije\n",
    "def ngk(string1, string2):\n",
    "    def ngrams(string):\n",
    "        ngrams = set(())\n",
    "        for n in range(1, len(string)+1):\n",
    "            ngrams_helper = zip(*[string[i:] for i in range(n)])\n",
    "            for ngram in ngrams_helper:\n",
    "                ngrams.add(''.join(ngram))\n",
    "        #print(ngrams)\n",
    "        return ngrams\n",
    "    \n",
    "    ngrams_1 = ngrams(string1) # racuna ngrams za prvi dokument\n",
    "    ngrams_2 = ngrams(string2) # racuna ngrams za drugi dokument\n",
    "    \n",
    "    # usporeduje broj jednakih ngrams oba dokumenta\n",
    "    intercept_rez = ngrams_1.intersection(ngrams_2)\n",
    "    num_common = len(intercept_rez)\n",
    "    \n",
    "    rez = num_common/(len(ngrams_1)+len(ngrams_2))\n",
    "    rez = rez/0.5 #skaliranje\n",
    "    return rez\n",
    "\n",
    "def ngk_kernel(X1, X2):\n",
    "    kernel_matrix = np.zeros([len(X1), len(X2)])\n",
    "    for i in range(0, len(X1)):\n",
    "        for j in range(0, len(X2)):\n",
    "            kernel_matrix[i][j] = ngk(X1[i], X1[j])\n",
    "    return kernel_matrix\n",
    "\n",
    "print(ngk_kernel(\"car\",\"cat\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "52ff7c03b8138598d26f7db6d70fd0156ea4d6d9d9b70d008337a8032a894406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
